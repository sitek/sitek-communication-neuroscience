[
  {
    "objectID": "talk.html#whats-special-about-the-auditory-system",
    "href": "talk.html#whats-special-about-the-auditory-system",
    "title": "Imaging the human auditory system",
    "section": "What’s special about the auditory system?",
    "text": "What’s special about the auditory system?\n\n\n\n\n\n\n\nKaas & Hackett (2000)\n\n\n\n\n\n\n\n\n\nVan Essen et al. (1992)\n\n\n\n\n\n\n\nSo why should we focus on the subcortical parts of the auditory system? I said that those are the harder parts to investigate with MRI, right? Well, there’s actually a lot going on in the brainstem and thalamus, especially in the auditory system. Two examples from primates show differences between the wiring of the auditory and visual systems. In the auditory system, the first 5 layers of the system are subcortical, and with some cross-talk between them, so some processing can pass through even more structures. Meanwhile, in the visual system, there’s just one or two stages before hitting primary sensory cortex. Indeed, a lot of processing is already done by the retina itself. With other features extracted later in successively higher levels of visual cortex. Whereas in the auditory system, there’s some basic frequency and sound level information pulled out by the cochlea, but some other features like location of sound sources are extracted in the brainstem.\nBUT THE VAST MAJORITY OF OUR KNOWLEDGE COMES FROM ANIMAL STUDIES"
  },
  {
    "objectID": "talk.html#the-human-subcortical-auditory-system",
    "href": "talk.html#the-human-subcortical-auditory-system",
    "title": "Imaging the human auditory system",
    "section": "The human subcortical auditory system",
    "text": "The human subcortical auditory system\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut: small, deep brain structures are challenging to image.\n\n\nAcross human behaviors, studying the brainstem is challenging. - In vivo MRI limited by: - high resolution needed for small structures - decreased signal as voxels get smaller - decreased signal in brainstem - Limited spatial information from EEG, MEG - Some human histology studies (nuclei) - Tracer studies (connections) only in animal models"
  },
  {
    "objectID": "talk.html#imaging-the-human-subcortical-auditory-system",
    "href": "talk.html#imaging-the-human-subcortical-auditory-system",
    "title": "Imaging the human auditory system",
    "section": "Imaging the human subcortical auditory system",
    "text": "Imaging the human subcortical auditory system\n\n\n\n\n\n\n\nSubcortical system responds more strongly to loud broadband sounds (Sigalovsky & Melcher, 2006)\n\n\n\n\n\n\n\n\n\nSubcortical auditory system responds tonotopically (7T fMRI; Moerel et al., 2015)\n\n\n\n\n\n\n\n\n\nProbabilistic pathways reaching auditory thalamus (Devlin et al., 2006)\n\n\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#my-contributions",
    "href": "talk.html#my-contributions",
    "title": "Imaging the human auditory system",
    "section": "My contributions",
    "text": "My contributions\n\n\nMapping the human subcortical auditory system\n\nAnatomy\nFunction\nConnectivity (structural and functional)\n\n\nBehavior of the human auditory system\n\nAuditory–motor interactions across the auditory pathway\nAuditory categorization in the human brain\nDeveloping speech-in-noise processes\n\n\n\n\nSpeaker notes go here.\n\n\nThis work is actively supported by K01-DC019421 and R21-DC022906."
  },
  {
    "objectID": "talk.html#brainstem-anatomy-gold-standard-vs.-standard-practice",
    "href": "talk.html#brainstem-anatomy-gold-standard-vs.-standard-practice",
    "title": "Imaging the human auditory system",
    "section": "Brainstem anatomy: Gold standard vs. standard practice",
    "text": "Brainstem anatomy: Gold standard vs. standard practice\n\n\n\n\n\n\n\nHistology (Paxinos et al., 2019)\n\n\n\n\n\n\n\n\n\nIn vivo 7T MRI (Sitek & Gulban et al., 2019)"
  },
  {
    "objectID": "talk.html#where-are-the-auditory-structures-in-humans",
    "href": "talk.html#where-are-the-auditory-structures-in-humans",
    "title": "Imaging the human auditory system",
    "section": "Where are the auditory structures in humans?",
    "text": "Where are the auditory structures in humans?\n\nPost mortem localization\n\nHuman histology: BigBrain (20 µm)\nPost mortem brainstem MRI (small-bore 7 Tesla Bruker MRI)\n\nT2*-weighted anatomy (50 µm)\nDiffusion-weighted MRI (200 µm)\n\n\n\n\nIn vivo localization\n\n7 Tesla functional MRI (Siemens Magnetom; 1.1 mm voxels)\n\n168 natural sounds (1 s each); sound &gt; baseline contrast\nCollected over 2 sessions"
  },
  {
    "objectID": "talk.html#human-histology-mapping",
    "href": "talk.html#human-histology-mapping",
    "title": "Imaging the human auditory system",
    "section": "Human histology mapping",
    "text": "Human histology mapping\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nSitek* and Gulban* et al. (2019 eLife)"
  },
  {
    "objectID": "talk.html#post-mortem-mri-mapping",
    "href": "talk.html#post-mortem-mri-mapping",
    "title": "Imaging the human auditory system",
    "section": "Post mortem MRI mapping",
    "text": "Post mortem MRI mapping\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#in-vivo-functional-mri-mapping",
    "href": "talk.html#in-vivo-functional-mri-mapping",
    "title": "Imaging the human auditory system",
    "section": "In vivo functional MRI mapping",
    "text": "In vivo functional MRI mapping\n\n\n7 Tesla functional MRI (1.1 mm voxels)\n\n168 natural sounds (1 s each): speech, voice, nature, tools, music, animals and monkey calls\nSound &gt; baseline contrast\n10 healthy participants\n2 hours per session\nCollected over 2 sessions\n\n\n\n\n\n\n\n\n\n\n\nParticipants listened to 168 natural sounds (1 s long) coming from seven categories (speech, voice, nature, tools, music, animals and monkey calls) presented in silent gaps in between the acquisition of functional volumes and were asked to press a button every time the same sound was repeated. The experimental paradigm followed a rapid-eventrelated design in which sounds were presented with a mean inter stimulus interval of four volumes (minimum three maximum five volumes). The two sessions were identical and each session consisted of twelve functional runs and across the 12 runs each sound was presented three times (i.e. each sounds was presented six times across the two sessions). The 168 sounds were divided in four sets of 42 sounds, each set was presented in three (non consecutive) runs. As a result, the 12 functional runs of each session formed four cross validation sets each one consisting of nine training runs and three testing runs (i.e. 126 training and 42 testing sounds). Note that the testing runs were non overlapping across the cross validations. Catch trials (i.e. sound repetitions) were added to each run, and were excluded from all analyses.\n\n\nSitek* and Gulban* et al. (2019 eLife)"
  },
  {
    "objectID": "talk.html#where-are-the-auditory-structures-public-atlases-from-three-sources",
    "href": "talk.html#where-are-the-auditory-structures-public-atlases-from-three-sources",
    "title": "Imaging the human auditory system",
    "section": "Where are the auditory structures: Public atlases from three sources",
    "text": "Where are the auditory structures: Public atlases from three sources\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nSitek* and Gulban* et al. (2019 eLife)"
  },
  {
    "objectID": "talk.html#anatomical-connectivity-mapping-with-diffusion-weighted-mri",
    "href": "talk.html#anatomical-connectivity-mapping-with-diffusion-weighted-mri",
    "title": "Imaging the human auditory system",
    "section": "Anatomical connectivity mapping with diffusion-weighted MRI",
    "text": "Anatomical connectivity mapping with diffusion-weighted MRI\n\n\n\n\nLook at how MRI signal varies across directions of molecular motion\nSince white matter constrains the motion of water molecules, we can infer orientation of white matter in each voxel\nThen traverse voxel-to-voxel to estimate pathways (tractography)\n\n\n\n\n\n\nwhole-brain diffusion tractography (HCP MGH Connectom data)\n\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#how-are-the-auditory-structures-connected-anatomically",
    "href": "talk.html#how-are-the-auditory-structures-connected-anatomically",
    "title": "Imaging the human auditory system",
    "section": "How are the auditory structures connected anatomically?",
    "text": "How are the auditory structures connected anatomically?\n\n\n\n\n\n\n\n\nPost mortem\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nSitek* and Gulban* et al. (2019 eLife); Sitek et al. (2022 Frontiers Neuro)"
  },
  {
    "objectID": "talk.html#how-are-the-auditory-structures-connected-anatomically-1",
    "href": "talk.html#how-are-the-auditory-structures-connected-anatomically-1",
    "title": "Imaging the human auditory system",
    "section": "How are the auditory structures connected anatomically?",
    "text": "How are the auditory structures connected anatomically?\n\n\n\n\n\n\nPost mortem\n\n\n\n\n\n\n\n\n\nIn vivo\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nSitek* and Gulban* et al. (2019 eLife); Sitek et al. (2022 Frontiers Neuro)"
  },
  {
    "objectID": "talk.html#how-are-the-auditory-structures-connected-functionally",
    "href": "talk.html#how-are-the-auditory-structures-connected-functionally",
    "title": "Imaging the human auditory system",
    "section": "How are the auditory structures connected functionally?",
    "text": "How are the auditory structures connected functionally?\n\n\n\n7T Human Connectome Project (106 participants)\n4 runs of resting state fMRI\n\n1 s x 900 TRs\n2 Anterior–Posterior runs, 2 Posterior–Anterior runs\n\nPartial correlations via novel autoregressive matrix-Gaussian copula graphical model\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nChandra* and Sitek* et al. (2024 Imaging Neuro)"
  },
  {
    "objectID": "talk.html#how-are-the-auditory-structures-connected-functionally-1",
    "href": "talk.html#how-are-the-auditory-structures-connected-functionally-1",
    "title": "Imaging the human auditory system",
    "section": "How are the auditory structures connected functionally?",
    "text": "How are the auditory structures connected functionally?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nChandra* and Sitek* et al. (2024 Imaging Neuro)"
  },
  {
    "objectID": "talk.html#how-is-the-brainstem-supplied-oxygen",
    "href": "talk.html#how-is-the-brainstem-supplied-oxygen",
    "title": "Imaging the human auditory system",
    "section": "How is the brainstem supplied oxygen?",
    "text": "How is the brainstem supplied oxygen?\n\n\n\n\n\n\n\nVessel ink staining (Duvernoy 1978)\n\n\n\n\n\n\n\nT2*-w MRI maximum intensity projection\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nOHBM 2023, 2024, 2025"
  },
  {
    "objectID": "talk.html#how-is-the-brainstem-supplied-oxygen-1",
    "href": "talk.html#how-is-the-brainstem-supplied-oxygen-1",
    "title": "Imaging the human auditory system",
    "section": "How is the brainstem supplied oxygen?",
    "text": "How is the brainstem supplied oxygen?\n\n\n\n\n\n\n\nVessel ink staining (Duvernoy 1978)\n\n\n\n\n\n\n\nT2*-w MRI maximum intensity projection\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nOHBM 2023, 2024, 2025"
  },
  {
    "objectID": "talk.html#but-what-if-we-dont-have-a-7t-mri",
    "href": "talk.html#but-what-if-we-dont-have-a-7t-mri",
    "title": "Imaging the human auditory system",
    "section": "But what if we don’t have a 7T MRI?",
    "text": "But what if we don’t have a 7T MRI?\nMulti-echo fMRI improves brainstem sensitivity at 3T\n\n\n\n\nMulti-echo fMRI acquisition\n\nEcho times = 13.40/39.5/65.6 ms\nRepetition time = 2.2 s\nMultiband factor = 2\nVoxel size = 1.731×1.731×4 mm^3\nWhole-brain coverage (44 slices, FOV=180mm)\n\nMulti-echo optimal combination + ICA noise removal (tedana)\n\n\n\n\n\n\n\n\n\n\nSo far, each of the results I’ve shown have used 7 Tesla MRI. These are becoming more prevalent, especially now that the Siemens Terra is approved by the FDA. But they’re still much less common than 3 Tesla MRIs. Fortunately, data acquisition protocols have improved sensitivty even at 3T, so that we can reliably identify auditory midbrain and thalamus.\n\n\nMedina et al. (in prep.; ISMRM 2025)"
  },
  {
    "objectID": "talk.html#but-what-if-we-dont-have-a-7t-mri-1",
    "href": "talk.html#but-what-if-we-dont-have-a-7t-mri-1",
    "title": "Imaging the human auditory system",
    "section": "But what if we don’t have a 7T MRI?",
    "text": "But what if we don’t have a 7T MRI?\nMulti-echo fMRI improves brainstem sensitivity at 3T\n\n\n\nHigh sensitivity across individuals\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigh reliability within individuals\n\n\n\n\n\n\n\n\nSo far, each of the results I’ve shown have used 7 Tesla MRI. These are becoming more prevalent, especially now that the Siemens Terra is approved by the FDA. But they’re still much less common than 3 Tesla MRIs. Fortunately, data acquisition protocols have improved sensitivty even at 3T, so that we can reliably identify auditory midbrain and thalamus.\n\n\nMedina et al. (in prep.; ISMRM 2025)"
  },
  {
    "objectID": "talk.html#human-speech-and-auditory-feedback-processing",
    "href": "talk.html#human-speech-and-auditory-feedback-processing",
    "title": "Imaging the human auditory system",
    "section": "Human speech and auditory feedback processing",
    "text": "Human speech and auditory feedback processing\n\n\nSuppressed cortical response to own voice during speaking vs. playback1\n\n\n\nMEG M100 speech-induced suppression\n\n\n\nIncreased BOLD in STG for self-generated tones2\n\n\n\n\n\n\n\nHoude; Niziolek; Chang;\n\nHoude & Jordan (2002)Reznik et al. (2014)"
  },
  {
    "objectID": "talk.html#cortical-feedback-processing-in-animals",
    "href": "talk.html#cortical-feedback-processing-in-animals",
    "title": "Imaging the human auditory system",
    "section": "Cortical feedback processing in animals",
    "text": "Cortical feedback processing in animals\n\n\nOverall suppression (but some excitation) in marmoset auditory cortex1\n\n\n\n\n\n\nMouse ACtx excitatory neurons inhibited, inhibitory neurons excited during movement2\n\n\n\n\n\n\n\na, An example neuron’s response to a preferred tone during rest (black) and movement (mvmt; red). b, The voltage area response of multiple neurons to preferred tone stimulus during rest versus movement (n = 27, P &lt; 0.001, paired t-test). c, Schematic showing viral infection of AAV-ChR2 into medial geniculate body (MGB) and optogenetic activation of ChR2+ axon terminals in the auditory cortex. d, Responses of an example neuron to optogenetic stimulation of thalamocortical terminals during rest (black) and movement (red). Blue bar indicates duration of light stimulation.\n\nEliades & Wang (2005)Schneider et al. (2014)"
  },
  {
    "objectID": "talk.html#subcortical-feedback-processing-in-animals",
    "href": "talk.html#subcortical-feedback-processing-in-animals",
    "title": "Imaging the human auditory system",
    "section": "Subcortical feedback processing in animals",
    "text": "Subcortical feedback processing in animals\n\n\nTop-down control\n\nCortical modulation accounts for some but not all of the auditory attenuation to self-generated sounds (mouse1)\nVocal pathways modulate efferent neurons to the inner ear and lateral line (fish2)\nFrequency-specific selective corticofugal modulation of individual olivocochlear efferent fibers (bat3)\n\n\nMouse licking in DCN4\n\n\n\n\n\n\n\nSpeaker notes go here.\n\nSchneider, Nelson, & Mooney 2014Weeg et al. 2005Xiao & Suga 2002Singla et al. (2017)"
  },
  {
    "objectID": "talk.html#where-are-motor-signals-integrated-subcortically",
    "href": "talk.html#where-are-motor-signals-integrated-subcortically",
    "title": "Imaging the human auditory system",
    "section": "Where are motor signals integrated? Subcortically?",
    "text": "Where are motor signals integrated? Subcortically?\n\n\n\n\nIf integration is subcortical, would expect to see FFR differences between self-generated vs. passive sound\n\nRecorded FFRs to /da/ stimulus (alternating polarity)\n\nActive condition: press a button to generate a sound\nPassive condion: passive presentation of the sound\n\nCollected data from 33 normal-hearing adults while they watched a silent captioned TV show\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf integration is subcortical, would expect to see FFR differences between self-generated vs. passive sound\n\n\nButler, …, Sitek (in prep; SNL 2025)"
  },
  {
    "objectID": "talk.html#where-are-motor-signals-integrated-subcortically-1",
    "href": "talk.html#where-are-motor-signals-integrated-subcortically-1",
    "title": "Imaging the human auditory system",
    "section": "Where are motor signals integrated? Subcortically?",
    "text": "Where are motor signals integrated? Subcortically?\n\nBrainstem frequency-following response (FFR) shows the same sound encoding when a sound is self-generated or passively presented\n\nIf integration is subcortical, would expect to see brainstem FFR differences between self-generated vs. passive sound\nNo brainstem FFR differences, despite cortical evoked response differences\nMotor signals unlikely to attenuate earliest central auditory signals\nNext steps: Source localization of FFR generators (ECR R21 project)\n\n\n\nIf integration is subcortical, would expect to see FFR differences between self-generated vs. passive sound\n\n\nButler, …, Sitek (in prep; SNL 2025)"
  },
  {
    "objectID": "talk.html#where-are-motor-signals-integrated-subcortically-2",
    "href": "talk.html#where-are-motor-signals-integrated-subcortically-2",
    "title": "Imaging the human auditory system",
    "section": "Where are motor signals integrated? Subcortically?",
    "text": "Where are motor signals integrated? Subcortically?\n\nBrainstem frequency-following response (FFR) shows the same sound encoding when a sound is self-generated or passively presented\n\nIf integration is subcortical, would expect to see brainstem FFR differences between self-generated vs. passive sound\nNo brainstem FFR differences, despite cortical evoked response differences\nMotor signals unlikely to attenuate earliest central auditory signals\nNext steps: Source localization of FFR generators (ECR R21 project)\n\n\n\nIf integration is subcortical, would expect to see FFR differences between self-generated vs. passive sound\n\n\nButler, …, Sitek (in prep; SNL 2025)"
  },
  {
    "objectID": "talk.html#where-are-motor-signals-integrated-subcortically-3",
    "href": "talk.html#where-are-motor-signals-integrated-subcortically-3",
    "title": "Imaging the human auditory system",
    "section": "Where are motor signals integrated? Subcortically?",
    "text": "Where are motor signals integrated? Subcortically?\n\nBrainstem frequency-following response (FFR) shows the same sound encoding when a sound is self-generated or passively presented\n\nIf integration is subcortical, would expect to see brainstem FFR differences between self-generated vs. passive sound\nNo brainstem FFR differences, despite cortical evoked response differences\nMotor signals unlikely to attenuate earliest central auditory signals\nNext steps: Source localization of FFR generators (ECR R21 project)\n\n\n\nIf integration is subcortical, would expect to see FFR differences between self-generated vs. passive sound\n\n\nButler, …, Sitek (in prep; SNL 2025)"
  },
  {
    "objectID": "talk.html#where-are-motor-signals-integrated-in-auditory-cortex",
    "href": "talk.html#where-are-motor-signals-integrated-in-auditory-cortex",
    "title": "Imaging the human auditory system",
    "section": "Where are motor signals integrated in auditory cortex?",
    "text": "Where are motor signals integrated in auditory cortex?\n\n\nPrevious literature implicates auditory cortex as a hub for auditory–motor integration\nBut fMRI has shown increased BOLD to self-generated sounds, vs. suppressed M/EEG responses1\nAre different laminar responses the explanation?\n\n\n\n\nWe can look at intra-cortical circuits with ultra-high field 7T fMRI\n\nDepth-dependent BOLD fMRI acquisition\n7 participants: 1 mm isotropic voxels, TE = 22.8 ms, MB = 6\n1 replication: 0.8 mm isotropic voxels, TE = 21.4, MB = 4\nCortical depth profile differences during self-generated vs. passive sound\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nSitek et al. (in prep; OHBM 2025)\nReznik et al. (2015)"
  },
  {
    "objectID": "talk.html#greater-self-generated-vs.-passive-fmri-responses-in-a-single-participant-across-replications",
    "href": "talk.html#greater-self-generated-vs.-passive-fmri-responses-in-a-single-participant-across-replications",
    "title": "Imaging the human auditory system",
    "section": "Greater self-generated vs. passive fMRI responses in a single participant across replications",
    "text": "Greater self-generated vs. passive fMRI responses in a single participant across replications\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nSitek et al. (in prep; OHBM 2025)"
  },
  {
    "objectID": "talk.html#stronger-self-generated-vs.-passive-sound-responses-in-superficial-auditory-cortex",
    "href": "talk.html#stronger-self-generated-vs.-passive-sound-responses-in-superficial-auditory-cortex",
    "title": "Imaging the human auditory system",
    "section": "Stronger self-generated (vs. passive) sound responses in superficial auditory cortex",
    "text": "Stronger self-generated (vs. passive) sound responses in superficial auditory cortex\n\n\n\nSpeaker notes go here.\n\n\nSitek et al. (in prep; OHBM 2025)"
  },
  {
    "objectID": "talk.html#evidence-for-layer-specific-motor-related-activation-in-human-auditory-cortex",
    "href": "talk.html#evidence-for-layer-specific-motor-related-activation-in-human-auditory-cortex",
    "title": "Imaging the human auditory system",
    "section": "Evidence for layer-specific motor-related activation in human auditory cortex",
    "text": "Evidence for layer-specific motor-related activation in human auditory cortex\n\n\n\nIncreased BOLD in self-generated vs. passive condition\nHigher increases at superficial cortical depths, suggesting increased corticocortical feedback\nHelps explain increased BOLD in previous studies\nReproducible—same cortical depth effects in two acquisitions in the same participant\n\n\n \n\n\n\nSpeaker notes go here.\n\n\nSitek et al. (in prep; OHBM 2025)"
  },
  {
    "objectID": "talk.html#pathways-for-auditory-learning-and-categorization",
    "href": "talk.html#pathways-for-auditory-learning-and-categorization",
    "title": "Imaging the human auditory system",
    "section": "Pathways for auditory learning and categorization",
    "text": "Pathways for auditory learning and categorization\n\n\n\n\n\n\n\nMacaque auditory-striatal connections1\n\n\n\n\n\n\n\n\n\nHuman auditory–striatal connections2\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\n\nYeterian & Pandya (1997)Sitek et al. (2022 preprint); Sitek et al. (submitted)"
  },
  {
    "objectID": "talk.html#how-do-humans-learn-new-sensory-categories",
    "href": "talk.html#how-do-humans-learn-new-sensory-categories",
    "title": "Imaging the human auditory system",
    "section": "How do humans learn new sensory categories?",
    "text": "How do humans learn new sensory categories?\n\n\n\n\nAuditory category learning task with Mandarin tone stimuli1\n12 non-Mandarin-speaking participants (18–30 years old)\n7 Tesla high resolution functional MRI (1.5 mm isotropic vixels)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\n\nYi 2016; Reetzke 2018; Feng 2019; …"
  },
  {
    "objectID": "talk.html#successful-learning-of-new-auditory-categories-in-mri",
    "href": "talk.html#successful-learning-of-new-auditory-categories-in-mri",
    "title": "Imaging the human auditory system",
    "section": "Successful learning of new auditory categories in MRI",
    "text": "Successful learning of new auditory categories in MRI\n\n\n\nSpeaker notes go here.\n\n\nSitek et al. (in revision)"
  },
  {
    "objectID": "talk.html#auditory-categorization-in-the-human-brain-1",
    "href": "talk.html#auditory-categorization-in-the-human-brain-1",
    "title": "Imaging the human auditory system",
    "section": "Auditory categorization in the human brain",
    "text": "Auditory categorization in the human brain\nWe identified putative cortico–striatal pathways for auditory decision-making\n\nPrimary/secondary auditory cortex connects more strongly with posterior dorsal striatum\nNon-invasive human connectivity is similar to post mortem tracer patterns in non-human primates (Yeterian & Pandya 1998)\n\nLearning a new auditory category broadly activates striatum and auditory cortex\n\nBoth striatum and cortex develop representations of task-relevant categories\nWhen presented with feedback:\n\nAnterior caudate responds highest early on\nAnterior putamen responds across learning stages\n\n\nPutamen and caudate subdivisions have unique functional and connectivity profiles\n\n\nSpeaker notes go here.\n\n\nSitek et al. (in revision)"
  },
  {
    "objectID": "talk.html#finger-motor-representation-in-the-human-brain",
    "href": "talk.html#finger-motor-representation-in-the-human-brain",
    "title": "Imaging the human auditory system",
    "section": "Finger motor representation in the human brain",
    "text": "Finger motor representation in the human brain\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior mean dominance greater than 25% across voxels for the right index (red), left index (blue), and middle fingers (green).\n\n\n\nMoya et al. Results for the fingertip mapping study:\nFingertip dominance across the investigated regions. Posterior mean dominance greater than \\(25\\%\\) is shown across voxels for the right index finger (red), left index (blue), and middle fingers (green) based on the estimated partition structures and the estimated effect intensities.\nColors blend when two or more fingers exhibit greater than \\(25\\%\\) dominance probabilities at the same location, showing areas of competing dominance for the right index and either middle finger (yellow), left index and either middle finger (cyan), and the two index fingers (purple).\n\n\nMoya et al. (in prep.)"
  },
  {
    "objectID": "talk.html#auditory-categorization-in-the-human-brain-2",
    "href": "talk.html#auditory-categorization-in-the-human-brain-2",
    "title": "Imaging the human auditory system",
    "section": "Auditory categorization in the human brain",
    "text": "Auditory categorization in the human brain\n\nLearning a new auditory categorization broadly activates striatum\nFeedback:\n\nAnterior caudate responds highest early on\nAnterior putamen responds across learning stages\n\nWe can use 7T fMRI to understand fine-grained striatal activity\n\n\n\nSpeaker notes go here.\n\n\nSitek et al. (in revision)"
  },
  {
    "objectID": "talk.html#the-developing-auditory-system",
    "href": "talk.html#the-developing-auditory-system",
    "title": "Imaging the human auditory system",
    "section": "The developing auditory system",
    "text": "The developing auditory system\nHow do we learn to hear in noisy environments?\n\n\n\n\nAdults: maturation of speech network1 + compensatory networks2\nChildren: fine-tuning speech network + undeveloped frontal networks\nCollected syllable-in-noise task fMRI at 3T in 35 typically developing kids\n\n5 speech-shaped noise contexts: Quiet and SNR = +8, 0, -2, and -6\nTask: identify which of 4 syllables were presented\n\n\n\n\n\n\n\nvia The Telegraph\n\n\n\n\n\n\n\nSSP\n\n\nSitek et al. (in prep.; SNL 2025)\nDu et al. (2014)Vaden et al. (2013); Eckert et al. (2016)"
  },
  {
    "objectID": "talk.html#the-developing-auditory-system-speech-in-quiet-vs.-baseline",
    "href": "talk.html#the-developing-auditory-system-speech-in-quiet-vs.-baseline",
    "title": "Imaging the human auditory system",
    "section": "The developing auditory system: Speech-in-quiet vs. baseline",
    "text": "The developing auditory system: Speech-in-quiet vs. baseline\n\n\n\n\n\n\n\nSSP\n\n\nSitek et al. (in prep.; SNL 2025)"
  },
  {
    "objectID": "talk.html#the-developing-auditory-system-speech-in-quiet-vs.--6-db-snr",
    "href": "talk.html#the-developing-auditory-system-speech-in-quiet-vs.--6-db-snr",
    "title": "Imaging the human auditory system",
    "section": "The developing auditory system: Speech-in-quiet vs. -6 dB SNR",
    "text": "The developing auditory system: Speech-in-quiet vs. -6 dB SNR\n\n\n\n\n\n\n\nSSP\n\n\nSitek et al. (in prep.; SNL 2025)"
  },
  {
    "objectID": "talk.html#systems-for-speech-in-noise-processing-are-actively-developing-in-adolescents",
    "href": "talk.html#systems-for-speech-in-noise-processing-are-actively-developing-in-adolescents",
    "title": "Imaging the human auditory system",
    "section": "Systems for speech-in-noise processing are actively developing in adolescents",
    "text": "Systems for speech-in-noise processing are actively developing in adolescents\n\n\n\n\nChildren more strongly activate broad canonical auditory and language regions in clear speech compared to noisy stimuli\nNo evidence that they utilize compensatory motor and prefrontal regions seen in adults\nInvestigating task-relevant representations in development\nCurrently collecting data in children with speech and language disorders\n\n\n\n\n\n\n\n\n\n\n\n\n\nSSP\n\n\nSpeaker notes go here.\n\n\nSitek et al. (in prep.; SNL 2025);\nHampton-Wray R01DC019904"
  },
  {
    "objectID": "talk.html#non-invasively-investigating-the-human-auditory-brainstem",
    "href": "talk.html#non-invasively-investigating-the-human-auditory-brainstem",
    "title": "Imaging the human auditory system",
    "section": "Non-invasively investigating the human auditory brainstem",
    "text": "Non-invasively investigating the human auditory brainstem\n\n\nMapping the human subcortical auditory system\n\n\nAuditory–motor interactions across the auditory pathway\n\n\nAuditory categorization in human dorsal striatum\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#acknowledgments",
    "href": "talk.html#acknowledgments",
    "title": "Imaging the human auditory system",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\n\nSoundBrain Lab\n\nBharath Chandrasekaran, Jacie McHaney, Kailyn McFarlane\nNike Gnanateja (now UW Madison)\nCasey Roark (now U New Hampshire)\n\n\n\nNorthwestern: Molly Bright, Michelle Medina (BME PhD student), Gabby Butler (Cog Sci UG honors student)\nUT Austin collaborators: Abhra Sarkar, Noirrit Chandra (now UT Dallas Statistics), Blake Moya (now in industry)\nPitt: Jay Bohland, Mandy Hampton Wray, Tamer Ibrahim (K01 co-mentor)\nHarvard/MIT: Satra Ghosh (PhD advisor)\nActive funding and support\n\nK01 DC019421 (2023–2027)\nR21 DC022906 (2025–2028)\nR01 DC020963 (2024–2027; PI: Bohland)\n\n\n\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#future-directions-hearing-loss",
    "href": "talk.html#future-directions-hearing-loss",
    "title": "Imaging the human auditory system",
    "section": "Future directions: Hearing loss",
    "text": "Future directions: Hearing loss\n\n\nSpeech-in-noise difficulties in middle age\nput an image here!\n\nNeural predictors of cochlear implant success\nput an image here!"
  },
  {
    "objectID": "talk.html#future-directions-analysis-methods",
    "href": "talk.html#future-directions-analysis-methods",
    "title": "Imaging the human auditory system",
    "section": "Future directions: Analysis methods",
    "text": "Future directions: Analysis methods\n\n\nffrprep\nput an image here!\n\nEx 2\nput an image here!"
  },
  {
    "objectID": "talk.html#science-is-not-a-silo",
    "href": "talk.html#science-is-not-a-silo",
    "title": "Imaging the human auditory system",
    "section": "Science is not a silo",
    "text": "Science is not a silo\n\n\nCommunicating our science:\n\nbuilds community\nstrengthens impact\nincreases resilience\n\n\nput an image here!\n\n\n\nSitek et al. (2025 Aperture Neuro); Wearn, Sitek, et al. (2025 Aperture Neuro)\n\n\n\n\nSitek et al. (2025 Aperture Neuro); Wearn, Sitek, et al. (2025 Aperture Neuro)"
  },
  {
    "objectID": "talk.html#improving-brainstem-sensitivity-at-3t-with-multi-echo-fmri",
    "href": "talk.html#improving-brainstem-sensitivity-at-3t-with-multi-echo-fmri",
    "title": "Imaging the human auditory system",
    "section": "Improving brainstem sensitivity at 3T with multi-echo fMRI",
    "text": "Improving brainstem sensitivity at 3T with multi-echo fMRI\n\n\n\n\nHow can we facilitate auditory pathway imaging?\n\nMost research scanners are 3T, not 7T\nPrevious auditory brainstem mapping at 3T used specialized approaches\n\n\n\nMulti-echo fMRI improves subcortical (and cortical!) fMRI sensitivity\n\nTypical fMRI uses one readout (“echo”)\nBut optimal echo time (TE) isn’t consistent across the brain\nMulti-echo fMRI allows optimal combination of different TEs across the brain\nAlso enables ICA noise removal across echoes (tedana)\n\n\n\n\n\n\n\nChandana Kodiweera, Ph.D. - Dartmouth BIC\n\n\n\n\n\n\n\nKundu et al. (2017)\n\n\n\n\n\n\nSo far, each of the results I’ve shown have used 7 Tesla MRI. These are becoming more prevalent, especially now that the Siemens Terra is approved by the FDA. But they’re still much less common than 3 Tesla MRIs. Fortunately, data acquisition protocols have improved sensitivty even at 3T, so that we can reliably identify auditory midbrain and thalamus.\n\nMulti-echo fMRI acquisition\n\nEcho times = 13.40/39.5/65.6 ms\nRepetition time = 2.2 s\nMultiband factor = 2\nVoxel size = 1.731×1.731×4 mm3\nWhole-brain coverage (44 slices, FOV=180mm)\n\nMulti-echo optimal combination + ICA noise removal (tedana)\n\n\n\nMedina et al. (preprint; ISMRM 2025)"
  },
  {
    "objectID": "talk.html#multi-echo-fmri-improves-group-level-brainstem-sensitivity-at-3t",
    "href": "talk.html#multi-echo-fmri-improves-group-level-brainstem-sensitivity-at-3t",
    "title": "Imaging the human auditory system",
    "section": "Multi-echo fMRI improves group-level brainstem sensitivity at 3T",
    "text": "Multi-echo fMRI improves group-level brainstem sensitivity at 3T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo far, each of the results I’ve shown have used 7 Tesla MRI. These are becoming more prevalent, especially now that the Siemens Terra is approved by the FDA. But they’re still much less common than 3 Tesla MRIs. Fortunately, data acquisition protocols have improved sensitivty even at 3T, so that we can reliably identify auditory midbrain and thalamus.\n\n\nMedina et al. (in prep.; ISMRM 2025)"
  },
  {
    "objectID": "talk.html#improved-within-participant-precision-mapping-with-multi-echo-fmri",
    "href": "talk.html#improved-within-participant-precision-mapping-with-multi-echo-fmri",
    "title": "Imaging the human auditory system",
    "section": "Improved within-participant precision-mapping with multi-echo fMRI",
    "text": "Improved within-participant precision-mapping with multi-echo fMRI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo far, each of the results I’ve shown have used 7 Tesla MRI. These are becoming more prevalent, especially now that the Siemens Terra is approved by the FDA. But they’re still much less common than 3 Tesla MRIs. Fortunately, data acquisition protocols have improved sensitivty even at 3T, so that we can reliably identify auditory midbrain and thalamus.\n\n\nMedina et al. (in prep.; ISMRM 2025)"
  },
  {
    "objectID": "talk.html#multi-echo-fmri-improves-group-level-sensitivity-at-3t",
    "href": "talk.html#multi-echo-fmri-improves-group-level-sensitivity-at-3t",
    "title": "Imaging the human auditory system",
    "section": "Multi-echo fMRI improves group-level sensitivity at 3T",
    "text": "Multi-echo fMRI improves group-level sensitivity at 3T\n\n\n\n\n\n\n\n\n\n\n\nMedina et al. (preprint; ISMRM 2025)"
  },
  {
    "objectID": "talk.html#improved-precision-mapping-with-multi-echo-fmri",
    "href": "talk.html#improved-precision-mapping-with-multi-echo-fmri",
    "title": "Imaging the human auditory system",
    "section": "Improved precision mapping with multi-echo fMRI",
    "text": "Improved precision mapping with multi-echo fMRI\n\n\n\nMulti-echo requires less scan time to identify auditory thalamus and brainstem\n\n\n\n\n\n\nSubcortical activations are stronger with multi-echo than single-echo fMRI\n\n\n\n\n\n\n\n\n\n\n\nMedina et al. (preprint; ISMRM 2025)"
  },
  {
    "objectID": "talk.html#improving-brainstem-sensitivity-at-3t-with-multi-echo-fmri-1",
    "href": "talk.html#improving-brainstem-sensitivity-at-3t-with-multi-echo-fmri-1",
    "title": "Imaging the human auditory system",
    "section": "Improving brainstem sensitivity at 3T with multi-echo fMRI",
    "text": "Improving brainstem sensitivity at 3T with multi-echo fMRI\n\n\n\nListening task in 3T fMRI\n\n\n14 participants\n10 minutes of pop-song listening\n3 participants returned for precision-mapping session\n\n\n\n\nMulti-echo fMRI acquisition from UMinn CMRR\n\nEcho times = 13.40/39.5/65.6 ms\nWhole-brain coverage with voxel size = 1.731×1.731×4 mm3\n\nMulti-echo optimal combination + ICA noise removal (tedana)\n\n\n\n\n\n\n\n\n\n\n\nSo far, each of the results I’ve shown have used 7 Tesla MRI. These are becoming more prevalent, especially now that the Siemens Terra is approved by the FDA. But they’re still much less common than 3 Tesla MRIs. Fortunately, data acquisition protocols have improved sensitivty even at 3T, so that we can reliably identify auditory midbrain and thalamus.\n\nMulti-echo fMRI acquisition\n\nEcho times = 13.40/39.5/65.6 ms\nRepetition time = 2.2 s\nMultiband factor = 2\nVoxel size = 1.731×1.731×4 mm3\nWhole-brain coverage (44 slices, FOV=180mm)\n\nMulti-echo optimal combination + ICA noise removal (tedana)\n\n\n\nMedina et al. (preprint; ISMRM 2025)"
  },
  {
    "objectID": "talk.html#improving-brainstem-sensitivity-at-3t-with-multi-echo-fmri-2",
    "href": "talk.html#improving-brainstem-sensitivity-at-3t-with-multi-echo-fmri-2",
    "title": "Imaging the human auditory system",
    "section": "Improving brainstem sensitivity at 3T with multi-echo fMRI",
    "text": "Improving brainstem sensitivity at 3T with multi-echo fMRI\nFuture applications: Auditory processing across the lifespan\n\n\nAuditory processing changes throughout adulthood\n\nHow does speech-in-noise processing change in middle age and beyond?\nWhat neural patterns predict later cochlear implant success?\n\n\nSpeech processing in the developing brain\n\nHow do children make auditory category decisions, and how are they affected by noise?\nWhat compensatory neural pathways are available to children with developmental communication differences?\n\n\n\n\n\n\nMoser et al. (2025)\n\n\n\n\nMoser et al. (2025): https://doi.org/10.1162/imag_a_00426 “The sample consists of one adult (PA001) and two children (PC001 and PC002; age 10) enrolled at the University of Minnesota (UMN), and one adult (PA002) and three healthy neonates enrolled at Washington University in St. Louis (WashU), ages 28 days (43 weeks postmenstrual age (wPMA); PB004), 12 days (41 wPMA; PB005), and 13 days (41 wPMA; PB001)”"
  },
  {
    "objectID": "talk.html#auditory-processing-changes-throughout-adulthood",
    "href": "talk.html#auditory-processing-changes-throughout-adulthood",
    "title": "Imaging the human auditory system",
    "section": "#### Auditory processing changes throughout adulthood",
    "text": "#### Auditory processing changes throughout adulthood"
  },
  {
    "objectID": "talk.html#speech-processing-in-the-developing-brain",
    "href": "talk.html#speech-processing-in-the-developing-brain",
    "title": "Imaging the human auditory system",
    "section": "#### Speech processing in the developing brain",
    "text": "#### Speech processing in the developing brain"
  },
  {
    "objectID": "talk.html#multi-echo-fmri-improves-group-level-sensitivity-at-3t-1",
    "href": "talk.html#multi-echo-fmri-improves-group-level-sensitivity-at-3t-1",
    "title": "Imaging the human auditory system",
    "section": "Multi-echo fMRI improves group-level sensitivity at 3T",
    "text": "Multi-echo fMRI improves group-level sensitivity at 3T\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMedina et al. (in prep.; ISMRM 2025)"
  },
  {
    "objectID": "talk.html#auditory-categorization-in-the-human-brain-3",
    "href": "talk.html#auditory-categorization-in-the-human-brain-3",
    "title": "Imaging the human auditory system",
    "section": "Auditory categorization in the human brain",
    "text": "Auditory categorization in the human brain\n\nLearning a new auditory categorization broadly activates striatum\nFeedback:\n\nAnterior caudate responds highest early on\nAnterior putamen responds across learning stages\n\nWe can use 7T fMRI to understand fine-grained striatal activity\n\n\n\nSpeaker notes go here.\n\n\nSitek et al. (in revision)"
  },
  {
    "objectID": "talk.html#auditory-fmri-responses-to-sound-presentation",
    "href": "talk.html#auditory-fmri-responses-to-sound-presentation",
    "title": "Imaging the human auditory system",
    "section": "Auditory fMRI responses to sound presentation",
    "text": "Auditory fMRI responses to sound presentation\n\n\n\nSound responses: everything significant except for ParsTri\n\n\nSitek et al. (in revision)"
  },
  {
    "objectID": "talk.html#representations-of-auditory-categories",
    "href": "talk.html#representations-of-auditory-categories",
    "title": "Imaging the human auditory system",
    "section": "Representations of auditory categories",
    "text": "Representations of auditory categories\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSound responses: everything significant except for ParsTri\n\n\nSitek et al. (in revision)"
  },
  {
    "objectID": "talk.html#representational-similarity-analysis-rsa-of-auditory-categories",
    "href": "talk.html#representational-similarity-analysis-rsa-of-auditory-categories",
    "title": "Imaging the human auditory system",
    "section": "Representational similarity analysis (RSA) of auditory categories",
    "text": "Representational similarity analysis (RSA) of auditory categories\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCortical RSA\n\nStronger Tone than Talker representation overall and for each region\nSignificantly stronger Tone representation in STGp vs. HG or PT\n\nStriatum RSA\n\nStronger Tone than Talker representation overall (t = = 2.93, p = .014)\nNo significant difference between subdivisions\n\n\n\n\nSound responses: everything significant except for ParsTri\n\n\nSitek et al. (in revision)"
  },
  {
    "objectID": "talk.html#fmri-responses-to-feedback-correct-vs.-wrong",
    "href": "talk.html#fmri-responses-to-feedback-correct-vs.-wrong",
    "title": "Imaging the human auditory system",
    "section": "fMRI responses to feedback (“correct” vs. “wrong”)",
    "text": "fMRI responses to feedback (“correct” vs. “wrong”)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStriatum feedback responses\n\nStronger responses overall in early vs. final learning stages (runs)\nAll regions showed positive responses in early learning stage\nOnly anterior putamen maintained positive responses across all runs\nOnly anterior caudate significantly decreased responses from early to final learning stages\n\n\n\n\nSound responses: everything significant except for ParsTri\n\n\nSitek et al. (in revision)"
  },
  {
    "objectID": "talk.html#auditory-striatal-pathways-for-learning-and-categorization",
    "href": "talk.html#auditory-striatal-pathways-for-learning-and-categorization",
    "title": "Imaging the human auditory system",
    "section": "Auditory-striatal pathways for learning and categorization",
    "text": "Auditory-striatal pathways for learning and categorization\n\n\n\n\n\n\n\nMacaque (Yeterian & Pandya 1997)\n\n\n\n\n\n\n\n\n\nHuman (Sitek et al. 2022 preprint, in revision)\n\n\n\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#how-do-we-learn-new-sensory-categories",
    "href": "talk.html#how-do-we-learn-new-sensory-categories",
    "title": "Imaging the human auditory system",
    "section": "How do we learn new sensory categories?",
    "text": "How do we learn new sensory categories?\n\n\n\n\nDorsal striatum contribute to learning new categories thanks to:\n\nits many-to-one projections from distributed cortical networks that subserve motor planning, feedback processing, and decision-making\nits role in task-relevant sensory processing (Brovelli et al., 2011; Ell, 2011; Groenewegen, 2003; Lim et al., 2014; Seger, 2008, 2013; Seger and Cincotta, 2005)\n\nDorsal striatum involved in auditory category learning tasks using fMRI (Feng et al., 2019; Lim et al., 2019; Yi et al., 2016)\n\nLittle is known about the anatomical pathways that connect the human auditory system to the striatum\nHow do subdivisions of dorsal striatum functionally contribute to category learning?\n\n\n\n\n\n\n\nLim et al., 2014\n\n\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#processing-variability-in-our-own-speech",
    "href": "talk.html#processing-variability-in-our-own-speech",
    "title": "Imaging the human auditory system",
    "section": "Processing variability in our own speech",
    "text": "Processing variability in our own speech\n\n\nRelative to a central production1\n\n\n\n\n\n\nRelative to the previous production2\n\n\n\n\n\n\n\n\n\nNiziolek et al. (2013)Sitek, …, Niziolek, & Ford (2013)"
  },
  {
    "objectID": "talk.html#the-developing-auditory-system-speech-in-quiet-vs.-baseline-fmri-contrast",
    "href": "talk.html#the-developing-auditory-system-speech-in-quiet-vs.-baseline-fmri-contrast",
    "title": "Imaging the human auditory system",
    "section": "The developing auditory system: Speech-in-quiet vs. baseline fMRI contrast",
    "text": "The developing auditory system: Speech-in-quiet vs. baseline fMRI contrast\n\n\n\n\n\n\n\nSSP\n\n\nSitek et al. (in prep.; SNL 2025)"
  },
  {
    "objectID": "talk.html#studying-the-human-subcortical-auditory-system-with-eeg",
    "href": "talk.html#studying-the-human-subcortical-auditory-system-with-eeg",
    "title": "Imaging the human auditory system",
    "section": "Studying the human subcortical auditory system with EEG",
    "text": "Studying the human subcortical auditory system with EEG\nThe scalp-recorded frequency-following response (FFR) shows the fidelity of incoming sound encoding\n\n\n\n\n\n\n\n\n\n\nCoffey et al. (2018)"
  },
  {
    "objectID": "talk.html#active-and-passive-evoked-responses",
    "href": "talk.html#active-and-passive-evoked-responses",
    "title": "Imaging the human auditory system",
    "section": "Active and passive evoked responses",
    "text": "Active and passive evoked responses\n\n\n\nBrainstem frequency-following response\n\n\nCortical evoked response\n\n\n\n\n\n\n\nButler, …, Sitek (in prep; SNL 2025)"
  },
  {
    "objectID": "talk.html#active-and-passive-spectrograms",
    "href": "talk.html#active-and-passive-spectrograms",
    "title": "Imaging the human auditory system",
    "section": "Active and passive spectrograms",
    "text": "Active and passive spectrograms\n\n\n\nBrainstem frequency-following response\n\n\n\n\n\n\nCortical evoked response\n\n\n\n\n\n\n\n\n\n\n\nButler, …, Sitek (in prep; SNL 2025)"
  },
  {
    "objectID": "talk.html#active-and-passive-pitch-tracking",
    "href": "talk.html#active-and-passive-pitch-tracking",
    "title": "Imaging the human auditory system",
    "section": "Active and passive pitch tracking",
    "text": "Active and passive pitch tracking\n\n\n\n\n\n\n\n\n\n\n\nButler, …, Sitek (in prep; SNL 2025)"
  },
  {
    "objectID": "talk.html#section",
    "href": "talk.html#section",
    "title": "Imaging the human auditory system",
    "section": "",
    "text": "Mapping the human subcortical auditory system\n\nAnatomy\nFunction\nConnectivity (structural and functional)\n\n\nBehavior of the human auditory system\n\nAuditory–motor interactions across the auditory pathway\nAuditory categorization in the human brain\nDeveloping speech-in-noise processes\n\n\n\n\nSpeaker notes go here.\n\n\nThis work is actively supported by K01-DC019421 and R21-DC022906."
  },
  {
    "objectID": "talk.html#where-are-the-auditory-structures-three-public-atlases",
    "href": "talk.html#where-are-the-auditory-structures-three-public-atlases",
    "title": "Imaging the human auditory system",
    "section": "Where are the auditory structures: Three public atlases",
    "text": "Where are the auditory structures: Three public atlases\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nSitek* and Gulban* et al. (2019 eLife)"
  },
  {
    "objectID": "talk.html#anatomical-connectivity-mapping",
    "href": "talk.html#anatomical-connectivity-mapping",
    "title": "Imaging the human auditory system",
    "section": "Anatomical connectivity mapping",
    "text": "Anatomical connectivity mapping\n\n\nDiffusion-weighted MRI tractography\n\nLook at how MRI signal varies across directions of molecular motion\nSince white matter constrains the motion of water molecules, we can infer orientation of white matter in each voxel\nThen traverse voxel-to-voxel to estimate pathways (tractography)\n\n\n\n\n\nwhole-brain diffusion tractography (HCP MGH Connectom data)\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#auditory-striatal-pathways-for-learning-and-categorization-1",
    "href": "talk.html#auditory-striatal-pathways-for-learning-and-categorization-1",
    "title": "Imaging the human auditory system",
    "section": "Auditory-striatal pathways for learning and categorization",
    "text": "Auditory-striatal pathways for learning and categorization\n\n\n\n\n\n\n\nMacaque1\n\n\n\n\n\n\n\n\n\nHuman2\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\n\nYeterian & Pandya (1997)Sitek et al. (2022 preprint); Sitek et al. (submitted)"
  },
  {
    "objectID": "talk.html#auditory-striatal-pathways-in-non-human-primates",
    "href": "talk.html#auditory-striatal-pathways-in-non-human-primates",
    "title": "Imaging the human auditory system",
    "section": "Auditory-striatal pathways in non-human primates",
    "text": "Auditory-striatal pathways in non-human primates\n\n\n\n\nMacaque (Yeterian & Pandya 1998)\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#auditory-striatal-pathways-in-the-human-brain",
    "href": "talk.html#auditory-striatal-pathways-in-the-human-brain",
    "title": "Imaging the human auditory system",
    "section": "Auditory-striatal pathways in the human brain",
    "text": "Auditory-striatal pathways in the human brain\n\n\n\n\nSitek et al. (in revision)\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#auditory-fmri-responses-to-sound-presentation-1",
    "href": "talk.html#auditory-fmri-responses-to-sound-presentation-1",
    "title": "Imaging the human auditory system",
    "section": "Auditory fMRI responses to sound presentation",
    "text": "Auditory fMRI responses to sound presentation\n\n\n\n\n\n\n\nCortical sound responses\n\n\n\n\n\n\n\n\n\nStriatal sound responses\n\n\n\n\n\n\n\n\nSound responses: everything significant except for ParsTri\n\n\nSitek et al. (in revision)"
  },
  {
    "objectID": "talk.html#similar-connectivity-patterns-in-humans-and-macaques",
    "href": "talk.html#similar-connectivity-patterns-in-humans-and-macaques",
    "title": "Imaging the human auditory system",
    "section": "Similar connectivity patterns in humans and macaques",
    "text": "Similar connectivity patterns in humans and macaques\n\n\n\n\n\n\n\nMacaque (Yeterian & Pandya 1998)\n\n\n\n\n\n\n\n\n\nHuman (Sitek et al. 2022 preprint, in revision)\n\n\n\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#are-there-representations-of-task-relevant-auditory-categories",
    "href": "talk.html#are-there-representations-of-task-relevant-auditory-categories",
    "title": "Imaging the human auditory system",
    "section": "Are there representations of task-relevant auditory categories?",
    "text": "Are there representations of task-relevant auditory categories?\n\n\n\n\n\nRSA\n\n\n\n\n\n\n\n\n\nCortical RSA\n\nStronger Tone than Talker representation overall and for each region\nSignificantly stronger Tone representation in STGp vs. HG or PT\n\nStriatum RSA\n\nStronger Tone than Talker representation overall (t = = 2.93, p = .014)\nNo significant difference between subdivisions\n\n\n\n\nSound responses: everything significant except for ParsTri\n\n\nSitek et al. (in revision)"
  },
  {
    "objectID": "talk.html#post-mortem-mri-mapping-1",
    "href": "talk.html#post-mortem-mri-mapping-1",
    "title": "Imaging the human auditory system",
    "section": "Post mortem MRI mapping",
    "text": "Post mortem MRI mapping\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "talk.html#post-mortem-anatomical-localization",
    "href": "talk.html#post-mortem-anatomical-localization",
    "title": "Imaging the human auditory system",
    "section": "Post mortem anatomical localization",
    "text": "Post mortem anatomical localization\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nSitek* and Gulban* et al. (2019 eLife)"
  },
  {
    "objectID": "talk.html#d-subcortical-atlases",
    "href": "talk.html#d-subcortical-atlases",
    "title": "Imaging the human auditory system",
    "section": "3D subcortical atlases",
    "text": "3D subcortical atlases\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpeaker notes go here.\n\n\nSitek* and Gulban* et al. (2019 eLife)"
  }
]