---
title: "Imaging the neural systems underlying human communication" 
author: "Kevin R. Sitek, Ph.D."
institute: Northwestern University
date: January 14, 2025

title-slide-attributes:
    data-background-image: /images/KevinSitek_postmortem-human-brainstem_auditory-tractography.png
    data-background-size: contain
    data-background-opacity: "0.25"

format:
  revealjs: 
    footer: "https://sitek.github.io/"
    slide-number: True
    chalkboard: 
      buttons: false
    preview-links: auto
    theme: [default, rp-theme.scss]
    height: 1080
    width: 1920
---

## A systems-level approach to human speech neuroscience {visibility="uncounted" .unlisted}
![](images/Sitek_overview/Slide1.png){fig-align="center" height='900' .fragment}

## A systems-level approach to human speech neuroscience {visibility="uncounted" .unlisted}
![](images/Sitek_overview/Slide2.png){fig-align="center" height='900'}

## A systems-level approach to human speech neuroscience {visibility="uncounted" .unlisted}
![](images/Sitek_overview/Slide3.png){fig-align="center" height='900'}

::: {.notes}
Established collaborations:
- Functional and structural connectivity in **persistent developmental stuttering** (Sitek et al., 2016; ongoing Pitt collaboration)
- Speech feedback control in individuals with **cerebellar stroke** (R01 Co-Investigator; PI: Bohland)
- White matter pathways in **autism spectrum disorder** (Boets et al., 2018)
- White matter integrity in **sensorineural hearing loss** (Tarabichi et al., 2018)
:::

# My path to speech, hearing, and the brain 

## First steps: Linguistics and Cognitive Science Undergraduate at UC Berkeley {visibility="uncounted"}
![](images/Sitek_academic-bio/Slide2.png){fig-align="center" height='900'}

::: aside
Sitek  & Johnson (2012 Chicago Linguistic Society). "Ipsilateral and contralateral phonetic context effects". 
:::

## Brain imaging research assistant at UCSF/San Francisco VA Medical Center {visibility="uncounted"}
![](images/Sitek_academic-bio/Slide3.png){fig-align="center" height='900'}

::: aside
Sitek et al. (2013). "Auditory cortex processes variation in our own speech". 
:::

## PhD in Speech and Hearing Bioscience and Technology at Harvard/MIT {visibility="uncounted"}
![](images/Sitek_academic-bio/Slide4.png){fig-align="center" height='900'}

::: aside
**NRSA F31** (2016–2018); Sitek et al. (2016), Connectivity and **stuttering** severity; Sitek and Gulban et al. (2019), "Mapping the human **subcortical auditory system**". 
:::

## Postdoctoral research in Neuroscience at Baylor College of Medicine {visibility="uncounted"}
![](images/Sitek_academic-bio/Slide5.png){fig-align="center" height='900'}

::: aside
Ultra-high field 9.4 Tesla functional MRI of the human midbrain 
:::

## Research Scientist in Communication Science and Disorders at the University of Pittsburgh {visibility="uncounted"}
![](images/Sitek_academic-bio/Slide6.png){fig-align="center" height='900'}

::: aside
**NIH K01** (2022–2027); Sitek et al. (2022) "Structural connectivity of human inferior colliculus subdivisions..." 
:::

## Now: Research Assistant Professor at Northwestern University {visibility="uncounted"}
![](images/Sitek_academic-bio/Slide7.png){fig-align="center" height='900'}

::: aside
**NIH R21** (2025–2028); auditory learning and auditory–motor interactions 
:::

## Widening perspectives and expanding opportunities through teaching and mentorship

## Widening perspectives and expanding opportunities through teaching and mentorship {visibility="uncounted"}

![](images/Sitek_teaching/Slide1.png){fig-align="center"}

::: {.notes}
- Speech Production (MIT/Harvard Teaching Assistant)
  - Lecture on articulatory phonetics
  - Bok Teaching Center Award for Teaching Excellence
- The human subcortical auditory system (Pitt AuD program; Pitt CSD undergrad program guest lectures)
- Subcortical contributions to language (UT Dallas guest lecture)
- Communicating Science (Northwestern CSD PhD program)
  - Tailoring your written/oral/visual presentations to your specific audience
  - Builds on my own work expanding access to high quality science (Wearn, Sitek, et al., 2025; Sitek et al., 2025)
  - 73% of students rated the course at the highest possible level

 - Northwestern PhD committee/co-mentorship
   - Serena Mon (CSD)
   - Michelle Medina (BME; Medina, Sitek, et al., *submitted*; Medina, ..., Sitek, *in prep*)
 - Northwestern undergraduate mentorship
   - Gabby Butler (Cognitive Science; Butler, ..., Sitek, *in prep*)
   - Max Chen (Cognitive Science/Computer Science)
 - Pitt AuD/undergraduate co-mentorship

As well as informal PhD/PD mentorship as faculty in the SoundBrain Lab, including writing group leadership and technical mentorship
:::

## Widening perspectives and expanding opportunities through teaching and mentorship {visibility="uncounted"}

![](images/Sitek_teaching/Slide2.png){fig-align="center"}

::: {.notes}
- Speech Production (MIT/Harvard Teaching Assistant)
  - Lecture on articulatory phonetics
  - Bok Teaching Center Award for Teaching Excellence
- The human subcortical auditory system (Pitt AuD program; Pitt CSD undergrad program guest lectures)
- Subcortical contributions to language (UT Dallas guest lecture)
- Communicating Science (Northwestern CSD PhD program)
  - Tailoring your written/oral/visual presentations to your specific audience
  - Builds on my own work expanding access to high quality science (Wearn, Sitek, et al., 2025; Sitek et al., 2025)
  - 73% of students rated the course at the highest possible level

 - Northwestern PhD committee/co-mentorship
   - Serena Mon (CSD)
   - Michelle Medina (BME; Medina, Sitek, et al., *submitted*; Medina, ..., Sitek, *in prep*)
 - Northwestern undergraduate mentorship
   - Gabby Butler (Cognitive Science; Butler, ..., Sitek, *in prep*)
   - Max Chen (Cognitive Science/Computer Science)
 - Pitt AuD/undergraduate co-mentorship

As well as informal PhD/PD mentorship as faculty in the SoundBrain Lab, including writing group leadership and technical mentorship
:::

## Widening perspectives and expanding opportunities through teaching and mentorship  {visibility="uncounted"}

![](images/Sitek_teaching/Slide3.png){fig-align="center"}

::: {.notes}
- Speech Production (MIT/Harvard Teaching Assistant)
  - Lecture on articulatory phonetics
  - Bok Teaching Center Award for Teaching Excellence
- The human subcortical auditory system (Pitt AuD program; Pitt CSD undergrad program guest lectures)
- Subcortical contributions to language (UT Dallas guest lecture)
- Communicating Science (Northwestern CSD PhD program)
  - Tailoring your written/oral/visual presentations to your specific audience
  - Builds on my own work expanding access to high quality science (Wearn, Sitek, et al., 2025; Sitek et al., 2025)
  - 73% of students rated the course at the highest possible level

 - Northwestern PhD committee/co-mentorship
   - Serena Mon (CSD)
   - Michelle Medina (BME; Medina, Sitek, et al., *submitted*; Medina, ..., Sitek, *in prep*)
 - Northwestern undergraduate mentorship
   - Gabby Butler (Cognitive Science; Butler, ..., Sitek, *in prep*)
   - Max Chen (Cognitive Science/Computer Science)
 - Pitt AuD/undergraduate co-mentorship

As well as informal PhD/PD mentorship as faculty in the SoundBrain Lab, including writing group leadership and technical mentorship
:::

## Widening perspectives and expanding opportunities through teaching and mentorship  {visibility="uncounted"}

![](images/Sitek_teaching/Slide4.png){fig-align="center"}

::: {.notes}
- Speech Production (MIT/Harvard Teaching Assistant)
  - Lecture on articulatory phonetics
  - Bok Teaching Center Award for Teaching Excellence
- The human subcortical auditory system (Pitt AuD program; Pitt CSD undergrad program guest lectures)
- Subcortical contributions to language (UT Dallas guest lecture)
- Communicating Science (Northwestern CSD PhD program)
  - Tailoring your written/oral/visual presentations to your specific audience
  - Builds on my own work expanding access to high quality science (Wearn, Sitek, et al., 2025; Sitek et al., 2025)
  - 73% of students rated the course at the highest possible level

 - Northwestern PhD committee/co-mentorship
   - Serena Mon (CSD)
   - Michelle Medina (BME; Medina, Sitek, et al., *submitted*; Medina, ..., Sitek, *in prep*)
 - Northwestern undergraduate mentorship
   - Gabby Butler (Cognitive Science; Butler, ..., Sitek, *in prep*)
   - Max Chen (Cognitive Science/Computer Science)
 - Pitt AuD/undergraduate co-mentorship

As well as informal PhD/PD mentorship as faculty in the SoundBrain Lab, including writing group leadership and technical mentorship
:::

## Research contributions to communication neuroscience

:::: {.columns}

::: {.column .fragment}
### Mapping the human subcortical auditory system
- **Anatomy** (Sitek & Gulban et al., 2019)
- **Function** (Sitek & Gulban et al., 2019; Medina, Sitek, et al., submitted preprint)
- **Structural and functional connectivity** (Sitek & Gulban et al., 2019; Sitek et al., 2022; Chandra* & Sitek* et al., 2024)
- **Neurovascular organization** (Xu et al., ISMRM 2025; Xu, ..., S. Bollmann*, Sitek*, in prep.)
:::
::: {.column .fragment}
### Behavior of the human auditory system
- **Auditory categorization** in the human brain (Sitek et al., in re-review; Llanos & Sitek et al., in re-review; Deschamps, Sitek, et al., in prep.)
- **Auditory–motor interactions** across the auditory pathway (Butler, ..., Sitek, in prep.)
- **Top–down modulation of predicted sound processing** across the auditory pathway (Ara et al., 2024)
- **Speech-in-noise processing** in the developing brain (Sitek et al., SNL 2025; in prep.)
:::
::::

::: aside
This work is actively supported by K01-DC019421 and R21-DC022906 and was previously supported by F31-DC015695.
:::

::: {.notes}
Speaker notes go here.
:::


# Research contributions: Auditory circuits throughout the human brain
::: {.notes}
Speaker notes go here.
:::

## What's special about the auditory system?

::: {layout="[60,40]"}

![Kaas & Hackett (2000)](images/schematic-auditorysystem.png){fig-align="center" height='800'  .fragment}

![Van Essen et al. (1992)](images/schematic-visualsystem.png){fig-align="center" height='800'  .fragment}

:::


::: {.notes}
So why should we focus on the subcortical parts of the auditory system? I said that those are the harder parts to investigate with MRI, right? Well, there’s actually a lot going on in the brainstem and thalamus, especially in the auditory system.
Two examples from primates show differences between the wiring of the auditory and visual systems. In the auditory system, the first 5 layers of the system are subcortical, and with some cross-talk between them, so some processing can pass through even more structures.
Meanwhile, in the visual system, there’s just one or two stages before hitting primary sensory cortex. Indeed, a lot of processing is already done by the retina itself. With other features extracted later in successively higher levels of visual cortex.
Whereas in the auditory system, there’s some basic frequency and sound level information pulled out by the cochlea, but some other features like location of sound sources are extracted in the brainstem.

BUT THE VAST MAJORITY OF OUR KNOWLEDGE COMES FROM ANIMAL STUDIES
:::


## The human subcortical auditory system

:::: {.columns}

::: {.column}
![](images/Rotation_brainstem_and_thalamus.gif){fig-align="center" height=800 .fragment}
:::

::: {.column}
![](images/Auditory_Pathway.png){fig-align="center" height=800 .fragment}
:::

::::

::: {.fragment}
### But: small, deep brain structures are challenging to image.
:::

::: {.notes}
Across human behaviors, studying the brainstem is challenging.
- In vivo MRI limited by:
 - high resolution needed for small structures
 - decreased signal as voxels get smaller
 - decreased signal in brainstem
- Limited spatial information from EEG, MEG
- Some human histology studies (nuclei)
- Tracer studies (connections) only in animal models
:::

## Imaging the human subcortical auditory system {.smaller visibility="hidden"}

::: {.incremental}


::: {layout="[[1,1,1]]"  layout-valign="bottom"}
![Subcortical system responds more strongly to loud broadband sounds (Sigalovsky & Melcher, 2006)](images/SigalovskyMelcher2006.png){.fragment}

![Subcortical auditory system responds tonotopically (7T fMRI; Moerel et al., 2015)](images/Moerel2015.png){.fragment}

![Probabilistic pathways reaching auditory thalamus (Devlin et al., 2006)](images/Devlin2006_combined.png){.fragment}
:::

::: 

::: {.notes}
Speaker notes go here.
:::

# Mapping the human subcortical auditory system

::: {.notes}
Speaker notes go here.
:::

## Brainstem anatomy: Gold standard vs. standard practice {visibility="hidden"}

::: {layout-ncol=2}
![Histology (Paxinos et al., 2019)](images/Paxinos2019.png){.fragment}

![In vivo 7T MRI (Sitek & Gulban et al., 2019)](images/SitekGulban2019_invivo.png){.fragment}

:::

## *Where* are the auditory structures in humans? {visibility="hidden"}

::: {.fragment}
### Post mortem localization 
- Human histology: BigBrain (20 µm)
- Post mortem brainstem MRI (small-bore 7 Tesla Bruker MRI)
  - T2*-weighted anatomy (50 µm)
  - Diffusion-weighted MRI (200 µm)
:::
::: {.fragment}
### In vivo localization 
- 7 Tesla functional MRI (Siemens Magnetom; 1.1 mm voxels)
  - 168 natural sounds (1 s each); sound > baseline contrast
  - Collected over 2 sessions
:::

## Post mortem anatomical localization

::: {layout-ncol=2}
![](images/SitekGulban2019_postmortem-1_labeled.png){height="420" .fragment}

![](images/SitekGulban2019_postmortem-2_labeled.png){height="420" .fragment}

:::

::: aside
Sitek and Gulban et al. (2019 *eLife*)
:::

::: {.notes}
Speaker notes go here.
:::


## 3D subcortical atlases

::: {layout-ncol=2}
![](images/SitekGulban2019/fig_literature_and_bigbrain.png){height="420" .fragment}

![](images/SitekGulban2019/fig_exvivo_atlas_3D_atlas.png){height="420" .fragment}

:::

![](images/people/Omer-Faruk-Gulban.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Sitek and Gulban et al. (2019 *eLife*)
:::

::: {.notes}
Speaker notes go here.
:::

## In vivo functional MRI mapping
:::: {.columns}
::: {.column width="27%" .fragment}
### 7 Tesla functional MRI (1.1 mm voxels)
- 168 natural sounds (1 s each): speech, voice, nature, tools, music, animals and monkey calls
- Sound > baseline contrast
- 10 healthy participants
- 2 hours per session
- Collected over 2 sessions
:::

::: {.column width="73%" .fragment}
![](images/SitekGulban2019/fig_invivo_statmaps_and_exvivo_vasculature.png){fig-align="center" height=900}
:::
::::
![](images/people/Omer-Faruk-Gulban.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Sitek and Gulban et al. (2019 *eLife*)
:::

::: {.notes}
Participants listened to 168 natural sounds (1 s long) coming from seven categories (speech, voice, nature, tools, music, animals and monkey calls) presented in silent gaps in between the acquisition of functional volumes and were asked to press a button every time the same sound was repeated. The experimental paradigm followed a rapid-eventrelated design in which sounds were presented with a mean inter stimulus interval of four volumes (minimum three maximum five volumes). The two sessions were identical and each session consisted of twelve functional runs and across the 12 runs each sound was presented three times (i.e. each sounds was presented six times across the two sessions). The 168 sounds were divided in four sets of 42 sounds, each set was presented in three (non consecutive) runs. As a result, the 12 functional runs of each session formed four cross validation sets each one consisting of nine training runs and three testing runs (i.e. 126 training and 42 testing sounds). Note that the testing runs were non overlapping across the cross validations. Catch trials (i.e. sound repetitions) were added to each run, and were excluded from all analyses.
:::

## *Where* are the auditory structures: Three public atlases 

::: {.r-stack}
{{< video images/SitekGulban2019/elife-48932-fig9-video1.mp4 width="1278" height="568">}}

![](images/SitekGulban2019/fig_MNI-videos_bigbrain_postmortem_invivo_statmap_invivo_overlap.png){.fragment fig-align="center"}
:::

![](images/people/Omer-Faruk-Gulban.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Sitek and Gulban et al. (2019 *eLife*)
:::

::: {.notes}
Speaker notes go here.
:::

# Connectivity of the human subcortical auditory system

::: {.notes}
Speaker notes go here.
:::

## Anatomical connectivity mapping {visibility="hidden"}

:::: {.columns  layout-valign="center"}

::: {.column .fragment}
### Diffusion-weighted MRI tractography
- Look at how MRI signal varies across directions of molecular motion 
- Since white matter constrains the motion of water molecules, we can infer orientation of white matter in each voxel
- Then traverse voxel-to-voxel to estimate pathways (*tractography*)
:::

::: {.column .fragment}
![whole-brain diffusion tractography (HCP MGH Connectom data)](images/Connectom_tractogram.png){}
:::
::::

::: {.notes}
Speaker notes go here.
:::

## How are the auditory structures connected **anatomically**?

:::: {columns}
::: {.column .fragment}
#### Post mortem
{{< video images/SitekGulban2019/elife-48932-fig3-video1.mp4 width="922" height="720" >}}
:::

::: {.column .fragment}
#### In vivo
{{< video images/SitekGulban2019/elife-48932-fig6-video1.mp4 width="922" height="720" >}}

:::
::::

![](images/people/Omer-Faruk-Gulban.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Sitek and Gulban et al. (2019 *eLife*); 
Sitek et al. (2022 *Frontiers Neuro*)
:::

::: {.notes}
![Post mortem](images/SitekGulban2019/fig_postmortem_streamlines_kevin_v2_faruk_v1_May31.png){fig-align="center" height=777} 
![In vivo](images/SitekGulban2019/fig_invivo_diffusion_schematic_heatmap+schematic_16May.png){fig-align="center" height=777 .fragment}
:::

## How are the auditory structures connected **functionally**?{.smaller}

#### Extracting the *pathway* (partial correlations) from the *network* (simple correlations) {.fragment}


:::: {columns}

::: {.column width="30%"}
![](images/ChandraSitek/FAC_fig-2_regions.png){.fragment}
:::

::: {.column width="70%" }
![](images/ChandraSitek/composite_Slide1.png){.fragment}

:::
::::

7T Human Connectome Project (106 participants) resting state **functional** MRI


![](images/people/NoirritChandra.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Chandra* and Sitek* et al. (2024 *Imaging Neuro*)
:::

::: {.notes}
Speaker notes go here.
:::

## How are the auditory structures connected **functionally**?{.smaller visibility="uncounted"}

#### Extracting the *pathway* (partial correlations) from the *network* (simple correlations)

:::: {columns}

::: {.column width="30%"}
![](images/ChandraSitek/FAC_fig-2_regions.png)
:::

::: {.column width="70%" }
![](images/ChandraSitek/composite_Slide2.png)

:::
::::

7T Human Connectome Project (106 participants) resting state **functional** MRI


![](images/people/NoirritChandra.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Chandra* and Sitek* et al. (2024 *Imaging Neuro*)
:::

::: {.notes}
Speaker notes go here.
:::

# 

# Auditory categorization in the human brain

::: {.notes}
Speaker notes go here.
:::

## How do we learn new sensory categories?

::: {layout="[70,30]"}

::: {.incremental}
- **Dorsal striatum** contribute to learning new categories thanks to its many-to-one projections from **distributed cortical networks** that subserve:
  - motor planning
  - **task-relevant sensory processing**
  - **feedback processing**
  - decision-making
- Dorsal striatum involved in auditory category learning tasks using fMRI (Feng et al., 2019; Lim et al., 2019; Yi et al., 2016)
  - What **anatomical pathways** that connect the human auditory system to the striatum?
  - How do *subdivisions* of dorsal striatum **functionally contribute** to category learning?
:::

![Lim et al., 2014](images/Lim_striatum-schematic.jpg){fig-align="center"}
:::

::: aside
This work is actively supported by K01-DC019421 (PI: Sitek)
:::


::: {.notes}
Speaker notes go here.
:::


## Auditory-striatal pathways in non-human primates

![Macaque (Yeterian & Pandya 1998)](images/YeterianPandya1997.png){fig-align="center" height="800" .fragment}

::: aside
:::

::: {.notes}
Speaker notes go here.
:::


## Auditory-striatal pathways in the human brain

![Sitek et al. (in revision)](images/Sitek2025_categorization/Sitek_tractography_streamlines.png){fig-align="center" height="800" .fragment}

::: aside
Human Connectome Project 7T diffusion-weighted MRI dataset (n = 100)
:::

::: {.notes}
Speaker notes go here.
:::


## Similar connectivity patterns in humans and macaques

::: {.incremental}

::: {layout="[30,70]"}

![Macaque (Yeterian & Pandya 1998)](images/YeterianPandya1997.png){fig-align="center" height="600" .fragment}

![Human (Sitek et al. 2022 preprint, in revision)](images/Sitek_corticostriatal_endpoints.png){fig-align="center" height="750" .fragment}

:::
:::

::: aside
:::

::: {.notes}
Speaker notes go here.
:::


## How do humans learn new sensory categories? {.smaller}

:::: {layout="[30, 35, 35]"}

::: {.incremental}
- Auditory category learning task with Mandarin tone stimuli^[Yi 2016; Reetzke 2018; Feng 2019; …]
- 12 **non**-Mandarin-speaking participants (18–30 years old)
- 7 Tesla high resolution functional MRI (1.5 mm isotropic vixels) 

:::

![](images/Sitek2025_categorization/Sitek_Fig2_spect.png){fig-align="center" height="650" .fragment}

![](images/Sitek2025_categorization/Sitek_Fig2_exp.png){fig-align="center" height="650" .fragment}


::::

::: aside
:::
::: {.notes}
Speaker notes go here.
:::


## Successful learning of new auditory categories in MRI

![](images/Sitek2025_categorization/behav_learning-stage_subp-1.svg){height="900" .fragment}

::: aside
Sitek et al. (in revision)
:::
::: {.notes}
Speaker notes go here.
:::

## Successful learning of new auditory categories in MRI {visibility="uncounted"}

![](images/Sitek2025_categorization/behav_learning-stage_subp-1-2.svg){height="900"}

::: aside
Sitek et al. (in revision)
:::
::: {.notes}
Speaker notes go here.
:::

## Successful learning of new auditory categories in MRI {visibility="uncounted"}

![](images/Sitek2025_categorization/Sitek_beh_learning-stage.svg){height="900"}

::: aside
Sitek et al. (in revision)
:::
::: {.notes}
Speaker notes go here.
:::

## Auditory fMRI responses to sound presentation 

![](images/Sitek2025_categorization/Sitek_sound-vs-baseline_slice.png){height="750" .fragment}


::: aside
Sitek et al. (in revision)
:::

::: {.notes}
Sound responses: everything significant except for ParsTri
:::


## Auditory fMRI responses to sound presentation {visibility="hidden"}

::: {layout-ncol=2 layout-valign="bottom"}
![Cortical sound responses](images/Sitek2025_categorization/Sitek_Figure1_left.png){height="700" .fragment}

![Striatal sound responses](images/Sitek2025_categorization/Sitek_Figure1_right.png){height="700" .fragment}

:::

::: aside
Sitek et al. (in revision)
:::

::: {.notes}
Sound responses: everything significant except for ParsTri
:::

## Are there *representations* of task-relevant auditory categories? {visibility="hidden"}

:::: {.columns}

::: {.column width="20%"  .fragment}
![RSA](images/Sitek2025_categorization/Sitek_RSA-models.png){height="800" fig-align="center"}
:::
::: {.column width="45%"  .fragment}
![](images/Sitek2025_categorization/Sitek_RSA-ROI-results_overall.png){height="850" fig-align="center"}
:::
::: {.column width="35%" .fragment}

#### Cortical RSA
  - Stronger Tone than Talker representation overall *and for each region*
  - Significantly stronger Tone representation in **STGp** vs. HG or PT

#### Striatum RSA
  - Stronger Tone than Talker representation overall (t = = 2.93, p = .014)
  - No significant difference between subdivisions

:::
::::

::: aside
Sitek et al. (in revision)
:::

::: {.notes}
Sound responses: everything significant except for ParsTri
:::


## fMRI responses to feedback ("correct" vs. "wrong")

:::: {.columns}
::: {.column width="50%"  .fragment}
![](images/Sitek2025_categorization/Sitek_feedback_striatal_all-runs.png){fig-align="center"}
:::
::: {.column width="50%"  .fragment}
![](images/Sitek2025_categorization/Sitek_feedback_striatal_final-vs-early.png){fig-align="center"}
:::
::::

:::: {.fragment}
#### Striatum feedback responses 
  - Stronger responses *overall* in early vs. final learning stages (runs)
  - All regions showed positive responses in *early* learning stage
  - Only **anterior putamen** *maintained* positive responses across all runs
  - Only **anterior caudate** *significantly decreased* responses from early to final learning stages
:::

::: aside
Sitek et al. (in revision)
:::

::: {.notes}
Sound responses: everything significant except for ParsTri
:::

## Auditory categorization in the human brain

:::  {.fragment}
#### We identified putative cortico–striatal pathways for auditory decision-making
- Primary/secondary auditory cortex connects more strongly with *posterior* dorsal striatum
- Non-invasive human connectivity is similar to post mortem tracer patterns in non-human primates (Yeterian & Pandya 1998)
:::
::: {.fragment}
#### Dynamics of learning and categorization in dorsal striatum
#### When presented with **feedback**:
- **Anterior caudate** responds highest early on
- **Anterior putamen** responds across learning stages
:::

### Putamen and caudate *subdivisions* have unique functional and connectivity profiles {.fragment}

::: aside
Sitek et al. (in revision)
:::

::: {.notes}
Speaker notes go here.
:::

#

# Auditory–motor interactions across the auditory pathway

::: {.notes}
So far I've focused on the basics: where are human auditory structures, and how are they connected. 
Ultimately, we want to *use* these approaches to learn more about the *function* of the human brain 
*in action*. 
:::

## Human speech and auditory feedback processing

:::: {.columns}

::: {.column .fragment}
#### Suppressed cortical response to own voice while speaking (Houde & Jordan 2002)

![](images/HoudeJordan2002.png){fig-align="center" height="700"}

:::

::: {.column .fragment}
#### Less suppression when our own speech is *variable* (Sitek et al. 2013)

![](images/Sitek2013_Fig1.png){fig-align="center" height="700"}

:::
::::


::: aside
This work is actively supported by R21-DC022906 (PI: Sitek).
:::

::: {.notes}
:::

## Auditory feedback processing in animals  {visibility="hidden"}

:::: {.columns}

::: {.column .fragment}
#### Overall suppression (but some excitation) in marmoset auditory cortex^[Eliades & Wang (2005)]

![](images/EliadesWang2005.png){height="500" fig-align="center"}
:::

::: {.column .fragment}
#### Decreased auditory brainstem response to own licking sound in mouse^[Singla et al. (2017)]
![](images/Singla2017.png){height="800" fig-align="center"}
:::

::::

::: {.notes}
Speaker notes go here.
:::

## Studying the human subcortical auditory system with EEG

#### The scalp-recorded frequency-following response (FFR) shows the fidelity of incoming sound encoding
![](images/Coffey2018_Fig1a.png){fig-align="center" .fragment}

::: aside
Coffey et al. (2018)
:::

::: {.notes}
:::

## *Where* are motor signals integrated? Subcortically? 

![](images/people/GabbyButler.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

:::: {layout="[60,40]"}

::: {.column .fragment}
- If integration is **subcortical**, would expect to see **FFR** differences between self-generated vs. passive sound
- Recorded FFRs to /da/ stimulus (alternating polarity)
  - *Active* condition: press a button to generate a sound
  - *Passive* condion: passive presentation of the sound
- Collected data from 33 normal-hearing adults while they watched a silent captioned TV show
:::

::: {.column .fragment}

![](images/Butler_etal_2025/Butler_EEG.png){fig-align="left" height="300"}

![](images/Butler_etal_2025/Butler_stimulus.png){fig-align="left" height="250"}

:::
::::

::: aside
Butler, ..., Sitek (in prep; SNL 2025)
:::

::: {.notes}
:::

## Active and passive evoked responses

![](images/people/GabbyButler.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

:::: {.columns}

::: {.column .fragment}
#### Brainstem frequency-following response
![](images/Butler_etal_2025/Butler_FFR_waveform.png){height=700}
:::

::: {.column .fragment}
#### Cortical evoked response
![](images/Butler_etal_2025/Butler_cortical_waveform.png){height=700}

:::
::::

::: aside
Butler, ..., Sitek (in prep; SNL 2025)
:::

::: {.notes}
:::

## Active and passive spectrograms {visibility="hidden"}

![](images/people/GabbyButler.jpeg){.absolute bottom=10 right=10 width="200" height="200"}


:::: {.columns}

::: {.column .fragment}
#### Brainstem frequency-following response
![](images/Butler_etal_2025/Butler_FFR_spectro.png){fig-align="center" height=450}
:::

::: {.column .fragment}
#### Cortical evoked response
![](images/Butler_etal_2025/Butler_cortical_spectro.png){fig-align="center" height=450}

:::
::::

::: aside
Butler, ..., Sitek (in prep; SNL 2025)
:::

::: {.notes}
:::

## Active and passive pitch tracking {visibility="hidden"}

![](images/people/GabbyButler.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

![](images/Butler_etal_2025/Butler_FFR_pitchtracking.png){.fragment height="750" fig-align="center"}

::: aside
Butler, ..., Sitek (in prep; SNL 2025)
:::

::: {.notes}
:::

## *Where* are motor signals integrated? Subcortically?

![](images/people/GabbyButler.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

#### Brainstem frequency-following response (FFR) shows the same sound encoding when a sound is self-generated or passively presented

::: {.incremental}
- No **brainstem FFR** differences, despite **cortical** evoked response differences
- *Motor signals unlikely to attenuate earliest central auditory signals*
- *Next steps*: 
  - Source localization of FFR generators (ECR R21 project)
  - Layer-specific cortical feedback localization (planned R01)
:::

::: aside
Butler, ..., Sitek (in prep; SNL 2025)
:::

::: {.notes}
If integration is subcortical, would expect to see FFR differences between self-generated vs. passive sound
:::

# Where we're going next

## Future research directions

![](images/Sitek_future-overview/Slide1.png){fig-align="center" height='900' .fragment}

::: {.notes}

Sensory *and motor speech* learning
- Role of striatum *across* communication functions
- Implications for stuttering, Parkinson's, other communication disorders
- R01 follow-up to K01

Hearing our own speech
- Layer-specific auditory processing of self-produced sounds (7T fMRI)
- MEG? Intracortical recordings?
- Implications for stuttering, autism, cerebellar stroke and ataxia of speech
- R01 follow-up to R21

Translational insights
- Adapting multi-echo fMRI to better understand **hearing loss in middle age** (Medina, Sitek, et al., *submitted*; Medina, ..., Sitek, *in prep*)
- Auditory cortical mechanisms supporting compensation in **children who stutter** (Sitek et al., SNL 2025, *in prep.*)
- Network connectivity supporting **aphasia** recovery


- ME-fMRI: collaborators in physical movement sciences are using this approach to understand brainstem pathways in motor recovery from stroke
- Stuttering:
- Cochlear implants: 
- Aphasia

:::

## Future research directions {visibility="uncounted"}

![](images/Sitek_future-overview/Slide2.png){fig-align="center" height='900'}

::: {.notes}

Sensory *and motor speech* learning
- Role of striatum *across* communication functions
- Implications for stuttering, Parkinson's, other communication disorders
- R01 follow-up to K01

Hearing our own speech
- Layer-specific auditory processing of self-produced sounds (7T fMRI)
- MEG? Intracortical recordings?
- Implications for stuttering, autism, cerebellar stroke and ataxia of speech
- R01 follow-up to R21

Translational insights
- Adapting multi-echo fMRI to better understand **hearing loss in middle age** (Medina, Sitek, et al., *submitted*; Medina, ..., Sitek, *in prep*)
- Auditory cortical mechanisms supporting compensation in **children who stutter** (Sitek et al., SNL 2025, *in prep.*)
- Network connectivity supporting **aphasia** recovery


- ME-fMRI: collaborators in physical movement sciences are using this approach to understand brainstem pathways in motor recovery from stroke
- Stuttering:
- Cochlear implants: 
- Aphasia

:::

## Future research directions {visibility="uncounted"}

![](images/Sitek_future-overview/Slide3.png){fig-align="center" height='900'}

::: {.notes}

Sensory *and motor speech* learning
- Role of striatum *across* communication functions
- Implications for stuttering, Parkinson's, other communication disorders
- R01 follow-up to K01

Hearing our own speech
- Layer-specific auditory processing of self-produced sounds (7T fMRI)
- MEG? Intracortical recordings?
- Implications for stuttering, autism, cerebellar stroke and ataxia of speech
- R01 follow-up to R21

Translational insights
- Adapting multi-echo fMRI to better understand **hearing loss in middle age** (Medina, Sitek, et al., *submitted*; Medina, ..., Sitek, *in prep*)
- Auditory cortical mechanisms supporting compensation in **children who stutter** (Sitek et al., SNL 2025, *in prep.*)
- Network connectivity supporting **aphasia** recovery


- ME-fMRI: collaborators in physical movement sciences are using this approach to understand brainstem pathways in motor recovery from stroke
- Stuttering:
- Cochlear implants: 
- Aphasia

:::

## Future research directions {visibility="uncounted"}

![](images/Sitek_future-overview/Slide4.png){fig-align="center" height='900'}

::: {.notes}
Sensory *and motor speech* learning
- Role of striatum *across* communication functions
- Implications for stuttering, Parkinson's, other communication disorders
- R01 follow-up to K01

Hearing our own speech
- Layer-specific auditory processing of self-produced sounds (7T fMRI)
- MEG? Intracortical recordings?
- Implications for stuttering, autism, cerebellar stroke and ataxia of speech
- R01 follow-up to R21

Translational insights
- Adapting multi-echo fMRI to better understand **hearing loss in middle age** (Medina, Sitek, et al., *submitted*; Medina, ..., Sitek, *in prep*)
- Auditory cortical mechanisms supporting compensation in **children who stutter** (Sitek et al., SNL 2025, *in prep.*)
- Network connectivity supporting **aphasia** recovery


- ME-fMRI: collaborators in physical movement sciences are using this approach to understand brainstem pathways in motor recovery from stroke
- Stuttering:
- Cochlear implants: 
- Aphasia

:::

## Sensory *and motor speech* learning: Shared striatal mechanisms?

:::: {.columns}

::: {.column width="50%"}
::: {.incremental}
- Dorsal striatum (caudate and putamen) is critical to auditory learning (Sitek et al., in revision) *and* motor learning
- Are the neural implementations shared across perceptual and motor learning **in the context of speech and language**?
- How are shared mechanisms affected in *stuttering*, *dyslexia*, or *Parkinson's*, and can they **inform therapeutic opportunities**?

:::
:::

::: {.column width="50%"}

![Lim et al., 2014](images/Lim_striatum-schematic.jpg){fig-align="center"}

:::
::::

::: {.notes}
Stuttering Treatment implications
Learning-based therapy, not just fluency shaping
Emphasize rewarded, gradually stabilized speech patterns
Closed-loop auditory reinforcement paradigms
More targeted dopamine modulation (dose, timing, task-coupled)
Neurostimulation aimed at striatal–SMA loops, not just cortex

Dyslexia Treatment implications
Shift from pure phonics to reinforcement-based learning frameworks
Use adaptive reward schedules to strengthen sound–action associations
Predict differential benefit from procedural vs declarative training
Early identification via auditory category learning tasks, not just phonological tests

Parkinson's Treatment implications
Therapy timed to dopaminergic state to optimize learning
Use of external reinforcement and cueing to replace striatal learning
Personalized speech therapy based on learning rate, not just severity
Speech as a biomarker for basal ganglia learning capacity

:::


## Hearing our own speech: *Where* are motor signals integrated in auditory cortex?

:::: {.columns}

::: {.column width="50%"}
::: {.incremental}
- Auditory cortex is a hub for motor integration
- But fMRI has shown *increased* BOLD to self-generated sounds, vs. *suppressed* M/EEG
- Are different **laminar** responses the explanation? 

#### We can look at intra-cortical circuits with ultra-high field **7T fMRI** {.fragment}

:::
:::

::: {.column width="50%"}

![Okada, Matchin, & Hickok (2018)](images/Matchin_artic-gt-imag_right.png){fig-align="center" height="150"}

![Reznik et al. (2015)](images/Reznik2014_Figure1a.png){fig-align="center" height="150"}

:::
::::

::: aside
:::

::: {.notes}
Speaker notes go here.
:::


## Hearing our own speech: *Where* are motor signals integrated in auditory cortex? {visibility="uncounted"}

:::: {.columns}

::: {.column width="50%"}
- Auditory cortex is a hub for motor integration
- But fMRI has shown *increased* BOLD to self-generated sounds, vs. *suppressed* M/EEG
- Are different **laminar** responses the explanation? 

#### We can look at intra-cortical circuits with ultra-high field **7T fMRI**

::: {.incremental}

- **Cortical depth profile** differences during self-generated vs. passive sound listening
- Higher increases at **superficial** cortical depths, suggesting increased **corticocortical** feedback
- Future work: Pinpointing feedback dysfunction in *stuttering*, *schizophrenia*, and other feedback processing disorders
:::
:::

::: {.column width="50%"}

![Okada, Matchin, & Hickok (2018)](images/Matchin_artic-gt-imag_right.png){fig-align="center" height="150"}

![Reznik et al. (2015)](images/Reznik2014_Figure1a.png){fig-align="center" height="150"}

![Sitek et al. (OHBM 2025)](images/selfgen.png){height="250"}
:::
::::

::: aside
:::

::: {.notes}
- Reproducible---same cortical depth effects in two acquisitions in the same participant
:::

## Improving clinical translation at 3T with multi-echo fMRI 

![](images/people/MichelleMedina.jpeg){.absolute bottom=10 right=10 width="200" height="200"}


:::: {.columns}

::: {.column}

::: {.fragment}
#### Multi-echo fMRI improves subcortical (and cortical!) fMRI sensitivity
- Typical fMRI uses one readout ("echo")
- But optimal echo time (TE) isn't consistent across the brain
- Multi-echo fMRI allows *optimal combination* of different TEs across the brain
:::

::: {.fragment}
#### Listening task in 3T fMRI
- 14 participants
- 10 minutes of pop-song listening
- 3 participants returned for precision-mapping session
- Multi-echo fMRI acquisition from UMinn CMRR
:::
:::

::: {.column}
![Chandana Kodiweera, Ph.D. - Dartmouth BIC](images/multiecho-vs-singleecho.png){fig-align="center"}
:::

::::


::: aside
Medina et al. (submitted preprint); Medina, ..., Sitek (in prep.)

:::

::: {.notes}
So far, each of the results I've shown have used 7 Tesla MRI. 
These are becoming more prevalent, especially now that the Siemens Terra is approved by the FDA. 
But they're still much less common than 3 Tesla MRIs. 
Fortunately, data acquisition protocols have improved sensitivty even at 3T, 
so that we can reliably identify auditory midbrain and thalamus.

- Multi-echo fMRI acquisition
  - Echo times = 13.40/39.5/65.6 ms
  - Repetition time = 2.2 s
  - Multiband factor = 2
  - Voxel size = 1.731×1.731×4 mm^3^
  - Whole-brain coverage (44 slices, FOV=180mm)
- Multi-echo optimal combination + ICA noise removal (`tedana`)
:::


## Improving brainstem sensitivity at 3T with multi-echo fMRI {visibility="hidden"}

![](images/people/MichelleMedina.jpeg){.absolute bottom=10 right=10 width="200" height="200"}


:::: {.columns}

::: {.column}
### Listening task in 3T fMRI
::: {.fragment}
- 14 participants
- 10 minutes of pop-song listening
- 3 participants returned for precision-mapping session
- Multi-echo fMRI acquisition from UMinn CMRR
:::
:::

::: {.column  .fragment}
![](images/Medina2025/Medina_task.png){fig-align="center" height=700}
:::

::::

::: aside
Medina et al. (submitted preprint); Medina, ..., Sitek (in prep.)

:::

::: {.notes}
- Multi-echo fMRI acquisition
  - Echo times = 13.40/39.5/65.6 ms
  - Repetition time = 2.2 s
  - Multiband factor = 2
  - Voxel size = 1.731×1.731×4 mm^3^
  - Whole-brain coverage (44 slices, FOV=180mm)
- Multi-echo optimal combination + ICA noise removal (`tedana`)
:::

## Multi-echo fMRI improves group-level sensitivity at 3T {visibility="hidden"}

![](images/people/MichelleMedina.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

:::: {.columns}

::: {.column}
![](images/Medina2025/Medina_cortex.png){.fragment fig-align="center"}
:::

::: {.column}
![](images/Medina2025/Medina_overview.png){.fragment fig-align="center"}
:::

::::

::: aside
Medina et al. (submitted preprint); Medina, ..., Sitek (in prep.)

:::

::: {.notes}
:::

## Improved precision mapping with multi-echo fMRI

![](images/people/MichelleMedina.jpeg){.absolute bottom=10 right=10 width="200" height="200"}


:::: {.columns}

::: {.column width="55%" .fragment}
#### Multi-echo requires less scan time to identify auditory brainstem
![](images/Medina2025/Medina_Figure4b_bstem.png){fig-align="center"  height="750"}
:::

::: {.column width="45%" .fragment}
#### Subcortical activations are stronger with multi-echo than single-echo fMRI
![](images/Medina2025/Medina_Figure4a.png){fig-align="center"}
:::

::::

::: aside
Medina et al. (submitted preprint); Medina, ..., Sitek (in prep.)

:::

::: {.notes}
:::


## Improving brainstem sensitivity at 3T with multi-echo fMRI 

### Future applications: Auditory processing across the lifespan

:::: {.columns}

::: {.column width="60%" .fragment}
#### Auditory processing changes throughout adulthood
- How does speech-in-noise processing change in middle age and beyond?
- What neural patterns predict later cochlear implant success?

::: {.fragment}
#### Speech processing in the developing brain
- How do children make auditory category decisions, and how are they affected by noise?
- What compensatory neural pathways are available to children with developmental communication differences?
:::
:::

::: {.column width="40%"}
![Moser et al. (2025)](images/imag_a_00426_fig4.jpeg){fig-align="center" height=800}
:::

::::


::: {.notes}
Moser et al. (2025): https://doi.org/10.1162/imag_a_00426
"The sample consists of one adult (PA001) and two children (PC001 and PC002; age 10) enrolled at the University of Minnesota (UMN), and one adult (PA002) and three healthy neonates enrolled at Washington University in St. Louis (WashU), ages 28 days (43 weeks postmenstrual age (wPMA); PB004), 12 days (41 wPMA; PB005), and 13 days (41 wPMA; PB001)"
:::

## Enhancing clinical precision with stronger human neuroscience {visibility="hidden"}

### Established collaborations
- Functional and structural connectivity in **persistent developmental stuttering** (Sitek et al., 2016; ongoing Pitt collaboration)
- Speech feedback control in individuals with **cerebellar stroke** (R01 Co-Investigator; PI: Bohland)
- White matter pathways in **autism spectrum disorder** (Boets et al., 2018)
- White matter integrity in **sensorineural hearing loss** (Tarabichi et al., 2018)

### R01 proposal: Neurobiology of middle-aged hearing loss
- *Basic neuroscience*: Quantify the relationship between noise exposure, hearing-in-noise difficulty, and structural changes to the cochlea, nerve, and cortex (in an animal model; Aravind Parthasarathy, Pitt)
- *Human neuroscience*: Identify structural, metabolic, and brain network relationships with noise exposure and speech-in-noise challenges (**Kevin Sitek**, imaging center TBD)
- *Translational neuroscience*: Develop speech-in-noise metrics and clinically applicable EEG to identify early hidden loss and provide therapeutic opportunities (Bharath Chandrasekaran, Northwestern Center for Speech, Hearing, and Language Learning)

## A systems-level approach to human speech neuroscience
![](images/Sitek_overview/Slide3.png){fig-align="center" height='900'}


## Acknowledgments {.smaller}

:::: {.columns}

::: {.column width="50%"}

#### SoundBrain Lab
- **Bharath Chandrasekaran**, Jacie McHaney, Kailyn McFarlane
- Nike Gnanateja (now UW Madison CSD)
- Casey Roark (now U New Hampshire Psychology)

![](images/people/SoundBrain-lab-current-members-and-alum_FFR-workshop-2024-550x310.jpg){height="500"}

:::

::: {.column width=50%"}
**Northwestern**: Molly Bright, **Michelle Medina** (*BME PhD student*), **Gabby Butler** (*Cog Sci UG honors student*)

**UT Austin collaborators**: Abhra Sarkar, Noirrit Chandra (*UT Dallas*), Blake Moya (*industry*)

**Pitt**: Jay Bohland, Mandy Hampton Wray, Tamer Ibrahim (*K01 co-mentor*)

**Harvard/MIT**: Satra Ghosh (*PhD advisor*)

### Active funding and support
- K01 DC019421 (2023–2027)
- R21 DC022906 (2025–2028)
- R01 DC020963 (2024–2027; PI: Bohland)

:::

::::

::: {.notes}
Speaker notes go here.
:::

# Thank you! {background-image="/images/KevinSitek_postmortem-human-brainstem_auditory-tractography.png" background-opacity=0.25 background-size='contain'}

# Extra slides {visibility="uncounted"}


::: {.notes}
Speaker notes go here.
:::


## How is the brainstem supplied oxygen? {visibility="uncounted"}


![](images/Xu2025/vasculature_T2w.png){.absolute top=10 right=10 width="300" height="300"}

::: {layout="[40,20,40]"  layout-valign="center" }
![Vessel ink staining (Duvernoy 1978)](images/Xu2025/vasculature_Duvernoy.png){height="750"}

![T2*-w MRI max. intensity projection](images/Xu2025/vasculature_MIP.png){height="750"}

{{< video images/Xu2025/S64520_m0_SLA_booster_result_bg-white.mp4  width="800%" height="500%" >}}
:::

![](images/people/MarshallXu.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Xu et al. (ISMRM 2025); Xu, ..., S. Bollmann*, Sitek* (in prep.)
:::

::: {.notes}
Video: 1920x1088
:::

## How is the brainstem supplied oxygen? {visibility="uncounted"}
{{< video images/Xu2025/S64520_m0_SLA_booster_result_bg-white.mp4  width="1920" height="1088" >}}



## Finger motor representation in the human brain {visibility="uncounted"}


::: {layout-ncol=3}
![](images/Moya_etal_2025/dmncADL.png)

![](images/Moya_etal_2025/dmncADR.png)

![](images/Moya_etal_2025/dmncPVR.png)
:::

Posterior mean dominance greater than 25% across voxels for the [right index  (red)]{style="color:red;"}, [left index (blue)]{style="color:blue;"}, and [middle fingers (green)]{style="color:green;"}. 

![](images/people/BlakeMoya.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Moya et al. (in prep.)
:::

::: {.notes}
Moya et al.
Results for the fingertip mapping study:     
Fingertip dominance across the investigated regions. Posterior mean dominance greater than $25\%$ is shown across voxels for the right index finger (red), left index (blue), and middle fingers (green) based on the estimated partition structures and the estimated effect intensities.     
Colors blend when two or more fingers exhibit greater than $25\%$ dominance probabilities at the same location,     showing areas of competing dominance for the right index and either middle finger (yellow), left index and either middle finger (cyan), and the two index fingers (purple).
:::


## The developing auditory system {visibility="uncounted"}


#### How do we learn to hear in noisy environments?

::: {layout-ncol=2  layout-valign="bottom"}

::: {.incremental}

- Adults: maturation of speech network^[Du et al. (2014)] + compensatory networks^[Vaden et al. (2013); Eckert et al. (2016)]
- Children: fine-tuning speech network + undeveloped frontal networks
- Collected syllable-in-noise task fMRI at 3T in 35 typically developing kids 
  - 5 speech-shaped noise contexts: Quiet and SNR = +8, 0, -2, and -6
  - Task: identify which of 4 syllables were presented
:::

![via The Telegraph](images/Cocktail-party.jpg){fig-align="center"}

:::

::: aside
Sitek et al. (in prep.; SNL 2025)
:::

::: {.notes}
SSP
:::

## The developing auditory system: Speech-in-quiet vs. baseline fMRI contrast {visibility="uncounted"}


![](images/Sitek_SSP_quiet-gt-baseline.png){fig-align="center" .fragment}

::: aside
Sitek et al. (in prep.; SNL 2025)
:::

::: {.notes}
SSP
:::

## The developing auditory system: Speech-in-quiet vs. -6 dB SNR {visibility="uncounted"}


![](images/Sitek_SSP_quiet-gt-n6.png){fig-align="center"}

::: aside
Sitek et al. (in prep.; SNL 2025)
:::

::: {.notes}
SSP
:::

## Systems for speech-in-noise processing are actively developing in adolescents {visibility="uncounted"}


::: {layout="[40,60]"   layout-valign="bottom"}
::: {.incremental}
- Children activate broad canonical auditory and language regions more strongly in clear speech compared to noisy stimuli
- No evidence that they utilize compensatory motor and prefrontal regions seen in adults
- Investigating task-relevant *representations* in development
- Currently collecting data in **children who stutter**
:::

![](images/Sitek_SSP_quiet-gt-n6.png){fig-align="center"}
:::

::: aside
Sitek et al. (in prep.; SNL 2025); 

Hampton-Wray R01DC019904
:::

::: {.notes}
SSP
:::


## Multi-echo fMRI improves group-level sensitivity at 3T {visibility="uncounted"}


![](images/people/MichelleMedina.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

![](images/Medina2025/Medina_Figure1.png){.fragment fig-align="center" height=900}

::: aside
Medina et al. (submitted preprint); Medina, ..., Sitek (in prep.)
:::

::: {.notes}
:::


::: {.notes}
Speaker notes go here.
:::

## Future directions: Hearing loss {visibility="hidden"}

:::: {.columns}

::: {.column width="50%"}

### Speech-in-noise difficulties in middle age 
*put an image here!*

:::

::: {.column width=50%"}
### Neural predictors of cochlear implant success
*put an image here!*
:::

::::

::: aside
:::

::: {.notes}
:::

## Future directions: Analysis methods {visibility="hidden"}

:::: {.columns}

::: {.column width="50%"}

### `ffrprep`
*put an image here!*

:::

::: {.column width=50%"}
### Ex 2
*put an image here!*
:::

::::

::: aside
:::

::: {.notes}
:::


# Communication in practice: Sharing our science {visibility="hidden"}

::: {.notes}
Speaker notes go here.
:::

## Science is not a silo {visibility="hidden"}

:::: {.columns}

::: {.column width="50%"}

Communicating our science:

- builds community
- strengthens impact
- increases resilience
:::

::: {.column width=50%"}
*put an image here!*
:::

::::

::: aside
Sitek et al. (2025 *Aperture Neuro*); 
Wearn, Sitek, et al. (2025 *Aperture Neuro*)
:::

::: {.notes}
Sitek et al. (2025 *Aperture Neuro*); 
Wearn, Sitek, et al. (2025 *Aperture Neuro*)
:::


## Greater self-generated vs. passive fMRI responses in a single participant across replications {visibility="uncounted"}

:::: {.columns}
::: {.column .fragment}
![](images/Sitek_MIS_slices_contrast-listen.png)
:::

::: {.column .fragment}
![](images/Sitek_MIS_slices_contrast-selfgen-vs-listen.png)
:::
::::

::: aside
Sitek et al. (OHBM 2025)
:::

::: {.notes}
Speaker notes go here.
:::

## Stronger self-generated (vs. passive) sound responses in superficial auditory cortex {visibility="uncounted"}


![](images/7T_replication.png)

::: aside
Sitek et al. (OHBM 2025)
:::

::: {.notes}
Speaker notes go here.
:::
