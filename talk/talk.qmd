---
title: "Imaging the human auditory system" 
subtitle: "Neuroanatomy, motor integration, and categorization"
author: "Kevin R. Sitek, Ph.D."
institute: Northwestern University
date: December 5, 2025

title-slide-attributes:
    data-background-image: /images/KevinSitek_postmortem-human-brainstem_auditory-tractography.png
    data-background-size: contain
    data-background-opacity: "0.25"

format:
  revealjs: 
    footer: "http://sitek.github.io/talk-auditory-imaging"
    slide-number: False
    chalkboard: 
      buttons: false
    preview-links: auto
    theme: [default, rp-theme.scss]
    height: 1080
    width: 1920
---


# The human auditory system
::: {.notes}
Speaker notes go here.
:::

## What's special about the auditory system?

::: {layout="[60,40]"}

![Kaas & Hackett (2000)](images/schematic-auditorysystem.png){fig-align="center" height='800'  .fragment}

![Van Essen et al. (1992)](images/schematic-visualsystem.png){fig-align="center" height='800'  .fragment}

:::


::: {.notes}
So why should we focus on the subcortical parts of the auditory system? I said that those are the harder parts to investigate with MRI, right? Well, there’s actually a lot going on in the brainstem and thalamus, especially in the auditory system.
Two examples from primates show differences between the wiring of the auditory and visual systems. In the auditory system, the first 5 layers of the system are subcortical, and with some cross-talk between them, so some processing can pass through even more structures.
Meanwhile, in the visual system, there’s just one or two stages before hitting primary sensory cortex. Indeed, a lot of processing is already done by the retina itself. With other features extracted later in successively higher levels of visual cortex.
Whereas in the auditory system, there’s some basic frequency and sound level information pulled out by the cochlea, but some other features like location of sound sources are extracted in the brainstem.

BUT THE VAST MAJORITY OF OUR KNOWLEDGE COMES FROM ANIMAL STUDIES
:::


## The human subcortical auditory system

:::: {.columns}

::: {.column}
![](images/Rotation_brainstem_and_thalamus.gif){fig-align="center" height=800 .fragment}
:::

::: {.column}
![](images/Auditory_Pathway.png){fig-align="center" height=800 .fragment}
:::

::::

::: {.fragment}
### But: small, deep brain structures are challenging to image.
:::

::: {.notes}
Across human behaviors, studying the brainstem is challenging.
- In vivo MRI limited by:
 - high resolution needed for small structures
 - decreased signal as voxels get smaller
 - decreased signal in brainstem
- Limited spatial information from EEG, MEG
- Some human histology studies (nuclei)
- Tracer studies (connections) only in animal models
:::

## Imaging the human subcortical auditory system {.smaller}

::: {.incremental}


::: {layout="[[1,1,1]]"  layout-valign="bottom"}
![Subcortical system responds more strongly to loud broadband sounds (Sigalovsky & Melcher, 2006)](images/SigalovskyMelcher2006.png){.fragment}

![Subcortical auditory system responds tonotopically (7T fMRI; Moerel et al., 2015)](images/Moerel2015.png){.fragment}

![Probabilistic pathways reaching auditory thalamus (Devlin et al., 2006)](images/Devlin2006_combined.png){.fragment}
:::

::: 

::: {.notes}
Speaker notes go here.
:::

## 

:::: {.columns}

::: {.column .fragment}
### Mapping the human subcortical auditory system
- Anatomy
- Function
- Connectivity (structural and functional)
:::
::: {.column .fragment}
### Behavior of the human auditory system
- Auditory–motor interactions across the auditory pathway
- Auditory categorization in the human brain
- Developing speech-in-noise processes
:::
::::

::: aside
This work is actively supported by K01-DC019421 and R21-DC022906.
:::

::: {.notes}
Speaker notes go here.
:::

# Mapping the human subcortical auditory system

::: {.notes}
Speaker notes go here.
:::

## Brainstem anatomy: Gold standard vs. standard practice

::: {layout-ncol=2}
![Histology (Paxinos et al., 2019)](images/Paxinos2019.png){.fragment}

![In vivo 7T MRI (Sitek & Gulban et al., 2019)](images/SitekGulban2019_invivo.png){.fragment}

:::

## *Where* are the auditory structures in humans?

::: {.fragment}
### Post mortem localization 
- Human histology: BigBrain (20 µm)
- Post mortem brainstem MRI (small-bore 7 Tesla Bruker MRI)
  - T2*-weighted anatomy (50 µm)
  - Diffusion-weighted MRI (200 µm)
:::
::: {.fragment}
### In vivo localization 
- 7 Tesla functional MRI (Siemens Magnetom; 1.1 mm voxels)
  - 168 natural sounds (1 s each); sound > baseline contrast
  - Collected over 2 sessions
:::

## Post mortem anatomical localization

::: {layout-ncol=2}
![](images/SitekGulban2019_postmortem-1_labeled.png){height="420" .fragment}

![](images/SitekGulban2019_postmortem-2_labeled.png){height="420" .fragment}

:::

::: aside
Sitek* and Gulban* et al. (2019 *eLife*)
:::

::: {.notes}
Speaker notes go here.
:::


## 3D subcortical atlases

::: {layout-ncol=2}
![](images/SitekGulban2019/fig_literature_and_bigbrain.png){height="420" .fragment}

![](images/SitekGulban2019/fig_exvivo_atlas_3D_atlas.png){height="420" .fragment}

:::

![](images/people/Omer-Faruk-Gulban.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Sitek* and Gulban* et al. (2019 *eLife*)
:::

::: {.notes}
Speaker notes go here.
:::

## In vivo functional MRI mapping
:::: {.columns}
::: {.column width="27%" .fragment}
### 7 Tesla functional MRI (1.1 mm voxels)
- 168 natural sounds (1 s each): speech, voice, nature, tools, music, animals and monkey calls
- Sound > baseline contrast
- 10 healthy participants
- 2 hours per session
- Collected over 2 sessions
:::

::: {.column width="73%" .fragment}
![](images/SitekGulban2019/fig_invivo_statmaps_and_exvivo_vasculature.png){fig-align="center" height=900}
:::
::::
![](images/people/Omer-Faruk-Gulban.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Sitek* and Gulban* et al. (2019 *eLife*)
:::

::: {.notes}
Participants listened to 168 natural sounds (1 s long) coming from seven categories (speech, voice, nature, tools, music, animals and monkey calls) presented in silent gaps in between the acquisition of functional volumes and were asked to press a button every time the same sound was repeated. The experimental paradigm followed a rapid-eventrelated design in which sounds were presented with a mean inter stimulus interval of four volumes (minimum three maximum five volumes). The two sessions were identical and each session consisted of twelve functional runs and across the 12 runs each sound was presented three times (i.e. each sounds was presented six times across the two sessions). The 168 sounds were divided in four sets of 42 sounds, each set was presented in three (non consecutive) runs. As a result, the 12 functional runs of each session formed four cross validation sets each one consisting of nine training runs and three testing runs (i.e. 126 training and 42 testing sounds). Note that the testing runs were non overlapping across the cross validations. Catch trials (i.e. sound repetitions) were added to each run, and were excluded from all analyses.
:::

## *Where* are the auditory structures: Three public atlases 

::: {.r-stack}
{{< video images/SitekGulban2019/elife-48932-fig9-video1.mp4 >}}

![](images/SitekGulban2019/fig_MNI-videos_bigbrain_postmortem_invivo_statmap_invivo_overlap.png){.fragment fig-align="center"}
:::

![](images/people/Omer-Faruk-Gulban.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Sitek* and Gulban* et al. (2019 *eLife*)
:::

::: {.notes}
Speaker notes go here.
:::

# Connectivity of the human subcortical auditory system

::: {.notes}
Speaker notes go here.
:::

## Anatomical connectivity mapping 

:::: {.columns  layout-valign="center"}

::: {.column .fragment}
### Diffusion-weighted MRI tractography
- Look at how MRI signal varies across directions of molecular motion 
- Since white matter constrains the motion of water molecules, we can infer orientation of white matter in each voxel
- Then traverse voxel-to-voxel to estimate pathways (*tractography*)
:::

::: {.column .fragment}
![whole-brain diffusion tractography (HCP MGH Connectom data)](images/Connectom_tractogram.png){}
:::
::::

::: {.notes}
Speaker notes go here.
:::

## How are the auditory structures connected **anatomically**?

::: {.incremental}

::: {layout-ncol=2}

::: {.r-stack}
{{< video images/SitekGulban2019/elife-48932-fig3-video1.mp4 width="600%" height="600%" >}}

![Post mortem](images/SitekGulban2019/fig_postmortem_streamlines_kevin_v2_faruk_v1_May31.png){fig-align="center" height=777 .fragment} 
:::

:::
:::
![](images/people/Omer-Faruk-Gulban.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Sitek* and Gulban* et al. (2019 *eLife*); 
Sitek et al. (2022 *Frontiers Neuro*)
:::

::: {.notes}
Speaker notes go here.
:::

## How are the auditory structures connected **anatomically**?


::: {layout-ncol=2}

![Post mortem](images/SitekGulban2019/fig_postmortem_streamlines_kevin_v2_faruk_v1_May31.png){fig-align="center" height=777} 

::: {.r-stack}
{{< video images/SitekGulban2019/elife-48932-fig6-video1.mp4 width="600%" height="600%" >}}

![In vivo](images/SitekGulban2019/fig_invivo_diffusion_schematic_heatmap+schematic_16May.png){fig-align="center" height=777 .fragment}
:::
:::
![](images/people/Omer-Faruk-Gulban.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Sitek* and Gulban* et al. (2019 *eLife*); 
Sitek et al. (2022 *Frontiers Neuro*)
:::

::: {.notes}
Speaker notes go here.
:::

## How are the auditory structures connected **functionally**?{.smaller}

:::: {columns}

::: {.column width="50%"}
::: {.incremental}
- 7T Human Connectome Project (106 participants)
- 4 runs of resting state fMRI
  - 1 s x 900 TRs
  - 2 Anterior–Posterior runs, 2 Posterior–Anterior runs
- Partial correlations via novel autoregressive matrix-Gaussian copula graphical model
:::
![](images/ChandraSitek/FAC_fig-2_main-result_schematic.png){height="400"}
:::

::: {.column width="25%" }
![](images/ChandraSitek/FAC_fig-2_main-result_AP.png){.fragment}
:::
::: {.column width="25%" }
![](images/ChandraSitek/FAC_fig-2_main-result_PA.png){.fragment}
:::

::::

![](images/people/NoirritChandra.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Chandra* and Sitek* et al. (2024 *Imaging Neuro*)
:::

::: {.notes}
Speaker notes go here.
:::

## How are the auditory structures connected **functionally**? {visibility="hidden"}

::: {layout="[50,25,25]"   layout-valign="center"}
![](images/ChandraSitek/FAC_fig-2_main-result_schematic.png){data-id="fig1"}

![](images/ChandraSitek/FAC_fig-2_main-result_AP.png){.fragment}

![](images/ChandraSitek/FAC_fig-2_main-result_PA.png){.fragment}
:::

![](images/people/NoirritChandra.jpg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Chandra* and Sitek* et al. (2024 *Imaging Neuro*)
:::

::: {.notes}
Speaker notes go here.
:::

# But what if we don't have a 7T MRI?

## Improving brainstem sensitivity at 3T with multi-echo fMRI 

![](images/people/MichelleMedina.jpeg){.absolute bottom=10 right=10 width="200" height="200"}


:::: {.columns}

::: {.column}
::: {.fragment}
#### How can we facilitate auditory pathway imaging? 
- Most research scanners are 3T, not 7T
- Previous auditory brainstem mapping at 3T used specialized approaches
:::

::: {.fragment}
#### Multi-echo fMRI improves subcortical (and cortical!) fMRI sensitivity
- Typical fMRI uses one readout ("echo")
- But optimal echo time (TE) isn't consistent across the brain
- Multi-echo fMRI allows *optimal combination* of different TEs across the brain
- Also enables ICA noise removal across echoes (`tedana`)
:::
:::

::: {.column}
![Chandana Kodiweera, Ph.D. - Dartmouth BIC](images/multiecho-vs-singleecho.png){fig-align="center" .fragment}

![Kundu et al. (2017)](images/physics_kundu_2017_multiple_echoes_3T-only.png){height=200 fig-align="left" .fragment}
:::

::::


::: aside
Medina et al. (preprint; ISMRM 2025)
:::

::: {.notes}
So far, each of the results I've shown have used 7 Tesla MRI. 
These are becoming more prevalent, especially now that the Siemens Terra is approved by the FDA. 
But they're still much less common than 3 Tesla MRIs. 
Fortunately, data acquisition protocols have improved sensitivty even at 3T, 
so that we can reliably identify auditory midbrain and thalamus.

- Multi-echo fMRI acquisition
  - Echo times = 13.40/39.5/65.6 ms
  - Repetition time = 2.2 s
  - Multiband factor = 2
  - Voxel size = 1.731×1.731×4 mm^3^
  - Whole-brain coverage (44 slices, FOV=180mm)
- Multi-echo optimal combination + ICA noise removal (`tedana`)
:::


## Improving brainstem sensitivity at 3T with multi-echo fMRI 

![](images/people/MichelleMedina.jpeg){.absolute bottom=10 right=10 width="200" height="200"}


:::: {.columns}

::: {.column}
### Listening task in 3T fMRI
::: {.fragment}
- 14 participants
- 10 minutes of pop-song listening
- 3 participants returned for precision-mapping session
:::

::: {.fragment}
- Multi-echo fMRI acquisition from UMinn CMRR
  - Echo times = 13.40/39.5/65.6 ms
  - Whole-brain coverage with voxel size = 1.731×1.731×4 mm^3^
- Multi-echo optimal combination + ICA noise removal (`tedana`)
:::
:::

::: {.column  .fragment}
![](images/Medina2025/Medina_task.png){fig-align="center" height=700}
:::

::::

::: aside
Medina et al. (preprint; ISMRM 2025)
:::

::: {.notes}
So far, each of the results I've shown have used 7 Tesla MRI. 
These are becoming more prevalent, especially now that the Siemens Terra is approved by the FDA. 
But they're still much less common than 3 Tesla MRIs. 
Fortunately, data acquisition protocols have improved sensitivty even at 3T, 
so that we can reliably identify auditory midbrain and thalamus.

- Multi-echo fMRI acquisition
  - Echo times = 13.40/39.5/65.6 ms
  - Repetition time = 2.2 s
  - Multiband factor = 2
  - Voxel size = 1.731×1.731×4 mm^3^
  - Whole-brain coverage (44 slices, FOV=180mm)
- Multi-echo optimal combination + ICA noise removal (`tedana`)
:::

## Multi-echo fMRI improves group-level sensitivity at 3T

![](images/people/MichelleMedina.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

![](images/Medina2025/Medina_Figure1.png){.fragment fig-align="center" height=900}

::: aside
Medina et al. (preprint; ISMRM 2025)
:::

::: {.notes}
:::

## Multi-echo fMRI improves group-level sensitivity at 3T {visibility="hidden"}

![](images/people/MichelleMedina.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

:::: {.columns}

::: {.column}
![](images/Medina2025/Medina_cortex.png){.fragment fig-align="center"}
:::

::: {.column}
![](images/Medina2025/Medina_overview.png){.fragment fig-align="center"}
:::

::::

::: aside
Medina et al. (preprint; ISMRM 2025)
:::

::: {.notes}
:::

## Improved precision mapping with multi-echo fMRI

![](images/people/MichelleMedina.jpeg){.absolute bottom=10 right=10 width="200" height="200"}


:::: {.columns}

::: {.column width="60%" .fragment}
#### Multi-echo requires less scan time to identify auditory thalamus and brainstem

![](images/Medina2025/Medina_Figure4b.png){fig-align="center"}
:::

::: {.column width="40%" .fragment}
#### Subcortical activations are stronger with multi-echo than single-echo fMRI
![](images/Medina2025/Medina_Figure4a.png){fig-align="center"}
:::

::::

::: aside
Medina et al. (preprint; ISMRM 2025)
:::

::: {.notes}
:::


## Improving brainstem sensitivity at 3T with multi-echo fMRI 

### Future applications: Auditory processing across the lifespan

:::: {.columns}

::: {.column .fragment}
#### Auditory processing changes throughout adulthood
- How does speech-in-noise processing change in middle age and beyond?
- What neural patterns predict later cochlear implant success?

::: {.fragment}
#### Speech processing in the developing brain
- How do children make auditory category decisions, and how are they affected by noise?
- What compensatory neural pathways are available to children with developmental communication differences?
:::
:::

::: {.column .fragment}
![Moser et al. (2025)](images/imag_a_00426_fig4.jpeg){fig-align="center" height=800}
:::

::::


::: {.notes}
Moser et al. (2025): https://doi.org/10.1162/imag_a_00426
"The sample consists of one adult (PA001) and two children (PC001 and PC002; age 10) enrolled at the University of Minnesota (UMN), and one adult (PA002) and three healthy neonates enrolled at Washington University in St. Louis (WashU), ages 28 days (43 weeks postmenstrual age (wPMA); PB004), 12 days (41 wPMA; PB005), and 13 days (41 wPMA; PB001)"
:::


# Auditory categorization in the human brain

::: {.notes}
Speaker notes go here.
:::

## How do we learn new sensory categories?

::: {layout="[70,30]"}

::: {.incremental}
- **Dorsal striatum** contribute to learning new categories thanks to:
  - its many-to-one projections from **distributed cortical networks** that subserve motor planning, feedback processing, and decision-making
  - its role in **task-relevant sensory processing** (Brovelli et al., 2011; Ell, 2011; Groenewegen, 2003; Lim et al., 2014; Seger, 2008, 2013; Seger and Cincotta, 2005)
- Dorsal striatum involved in auditory category learning tasks using fMRI (Feng et al., 2019; Lim et al., 2019; Yi et al., 2016)
  - Little is known about the **anatomical pathways** that connect the human auditory system to the striatum
  - How do *subdivisions* of dorsal striatum **functionally contribute** to category learning?
:::

![Lim et al., 2014](images/Lim_striatum-schematic.jpg){fig-align="center"}
:::

::: aside
:::

::: {.notes}
Speaker notes go here.
:::


## Auditory-striatal pathways in non-human primates

![Macaque (Yeterian & Pandya 1998)](images/YeterianPandya1997.png){fig-align="center" height="800" .fragment}

::: aside
:::

::: {.notes}
Speaker notes go here.
:::


## Auditory-striatal pathways in the human brain

![Sitek et al. (in revision)](images/Sitek2025_categorization/Sitek_tractography_streamlines.png){fig-align="center" height="800" .fragment}

::: aside
:::

::: {.notes}
Speaker notes go here.
:::


## Similar connectivity patterns in humans and macaques

::: {.incremental}

::: {layout="[30,70]"}

![Macaque (Yeterian & Pandya 1998)](images/YeterianPandya1997.png){fig-align="center" height="600" .fragment}

![Human (Sitek et al. 2022 preprint, in revision)](images/Sitek_corticostriatal_endpoints.png){fig-align="center" height="750" .fragment}

:::
:::

::: aside
:::

::: {.notes}
Speaker notes go here.
:::


## How do humans learn new sensory categories? {.smaller}

:::: {layout="[30, 35, 35]"}

::: {.incremental}
- Auditory category learning task with Mandarin tone stimuli^[Yi 2016; Reetzke 2018; Feng 2019; …]
- 12 **non**-Mandarin-speaking participants (18–30 years old)
- 7 Tesla high resolution functional MRI (1.5 mm isotropic vixels) 

:::

![](images/Sitek2025_categorization/Sitek_overview_stimuli.png){fig-align="center" height="700" .fragment}

![](images/Sitek2025_categorization/Sitek_overview_imaging.png){fig-align="center" height="700" .fragment}


::::

::: aside
:::
::: {.notes}
Speaker notes go here.
:::


## Successful learning of new auditory categories in MRI

![](images/Sitek2025_categorization/Sitek_beh_learning-stage.svg){height="900" .fragment}

::: aside
Sitek et al. (in revision)
:::
::: {.notes}
Speaker notes go here.
:::

## Auditory fMRI responses to sound presentation 

![](images/Sitek2025_categorization/Sitek_sound-vs-baseline_slice.png){height="750" .fragment}


::: aside
Sitek et al. (in revision)
:::

::: {.notes}
Sound responses: everything significant except for ParsTri
:::


## Auditory fMRI responses to sound presentation 

::: {layout-ncol=2 layout-valign="bottom"}
![Cortical sound responses](images/Sitek2025_categorization/Sitek_Figure1_left.png){height="700" .fragment}

![Striatal sound responses](images/Sitek2025_categorization/Sitek_Figure1_right.png){height="700" .fragment}

:::

::: aside
Sitek et al. (in revision)
:::

::: {.notes}
Sound responses: everything significant except for ParsTri
:::

## Are there *representations* of task-relevant auditory categories?

:::: {.columns}

::: {.column width="20%"  .fragment}
![RSA](images/Sitek2025_categorization/Sitek_RSA-models.png){height="800" fig-align="center"}
:::
::: {.column width="45%"  .fragment}
![](images/Sitek2025_categorization/Sitek_RSA-ROI-results_overall.png){height="850" fig-align="center"}
:::
::: {.column width="35%" .fragment}

#### Cortical RSA {.fragment}
  - Stronger Tone than Talker representation overall *and for each region*
  - Significantly stronger Tone representation in **STGp** vs. HG or PT

#### Striatum RSA {.fragment}
  - Stronger Tone than Talker representation overall (t = = 2.93, p = .014)
  - No significant difference between subdivisions

:::
::::

::: aside
Sitek et al. (in revision)
:::

::: {.notes}
Sound responses: everything significant except for ParsTri
:::


## fMRI responses to feedback ("correct" vs. "wrong")

:::: {.columns}
::: {.column width="50%"  .fragment}
![](images/Sitek2025_categorization/Sitek_feedback_striatal_all-runs.png){fig-align="center"}
:::
::: {.column width="50%"  .fragment}
![](images/Sitek2025_categorization/Sitek_feedback_striatal_final-vs-early.png){fig-align="center"}
:::
::::

:::: {.fragment}
#### Striatum feedback responses 
  - Stronger responses *overall* in early vs. final learning stages (runs)
  - All regions showed positive responses in *early* learning stage
  - Only **anterior putamen** *maintained* positive responses across all runs
  - Only **anterior caudate** *significantly decreased* responses from early to final learning stages
:::

::: aside
Sitek et al. (in revision)
:::

::: {.notes}
Sound responses: everything significant except for ParsTri
:::

## Auditory categorization in the human brain

::: {.incremental}
#### We identified putative cortico–striatal pathways for auditory decision-making {.fragment}
- Primary/secondary auditory cortex connects more strongly with *posterior* dorsal striatum
- Non-invasive human connectivity is similar to post mortem tracer patterns in non-human primates (Yeterian & Pandya 1998)

#### Learning a new auditory category broadly activates striatum and auditory cortex {.fragment}
- Both striatum and cortex develop **representations** of *task-relevant* categories
- When presented with **feedback**: 
  - **Anterior caudate** responds highest early on
  - **Anterior putamen** responds across learning stages

### Putamen and caudate *subdivisions* have unique functional and connectivity profiles {.fragment}
:::
::: aside
Sitek et al. (in revision)
:::

::: {.notes}
Speaker notes go here.
:::


# Auditory–motor interactions across the auditory pathway

::: {.notes}
So far I've focused on the basics: where are human auditory structures, and how are they connected. 
Ultimately, we want to *use* these approaches to learn more about the *function* of the human brain 
*in action*. 
:::

## Human speech and auditory feedback processing

:::: {.columns}

::: {.column .fragment}
#### Suppressed cortical response to own voice during speaking vs. playback^[Houde & Jordan (2002)]

![MEG M100 speech-induced suppression](images/HoudeJordan2002.png){fig-align="center" height="600"}

:::

::: {.column .fragment}

#### *Increased* BOLD in STG for self-generated tones^[Reznik et al. (2014)]

![](images/Reznik2014_Figure1.png){fig-align="center" height="600"}
:::
::::

::: {.notes}
Houde; Niziolek; Chang; 
:::

## Processing variability in our own speech

:::: {.columns}

::: {.column .fragment}
#### Relative to a central production^[Niziolek et al. (2013)]

![](images/Niziolek2013_Figure1.png){fig-align="center" height="700"}

:::

::: {.column .fragment}
#### Relative to the previous production^[**Sitek**, ..., Niziolek, & Ford (2013)]

![](images/Sitek2013_Fig1.png){fig-align="center" height="700"}

:::
::::

::: {.notes}
:::

## Cortical feedback processing in animals

:::: {.columns}

::: {.column .fragment}
#### Overall suppression (but some excitation) in marmoset auditory cortex^[Eliades & Wang (2005)]

![](images/EliadesWang2005.png){height="500" fig-align="center"}
:::

::: {.column .fragment}
#### Mouse ACtx excitatory neurons inhibited, inhibitory neurons excited during movement^[Schneider et al. (2014)]

![](images/Schneider2015_Fig2.png){fig-align="center"}
:::

::::

::: {.notes}
a, An example neuron’s response to a preferred tone during rest (black) and movement (mvmt; red). b, The voltage area response of multiple neurons to preferred tone stimulus during rest versus movement (n = 27, P < 0.001, paired t-test). c, Schematic showing viral infection of AAV-ChR2 into medial geniculate body (MGB) and optogenetic activation of ChR2+ axon terminals in the auditory cortex. d, Responses of an example neuron to optogenetic stimulation of thalamocortical terminals during rest (black) and movement (red). Blue bar indicates duration of light stimulation.
:::

## *Sub*cortical feedback processing in animals

:::: {.columns}

::: {.column .fragment}
#### Top-down control

- Cortical modulation accounts for some but not all of the auditory attenuation to self-generated sounds (mouse^[Schneider, Nelson, & Mooney 2014])
- Vocal pathways modulate efferent neurons to the inner ear and lateral line (fish^[Weeg et al. 2005])
- Frequency-specific selective corticofugal modulation of individual olivocochlear efferent fibers (bat^[Xiao & Suga 2002])

:::

::: {.column .fragment}
#### Mouse licking in DCN^[Singla et al. (2017)]
![](images/Singla2017.png){height="800" fig-align="center"}
:::

::::

::: {.notes}
Speaker notes go here.
:::

## Studying the human subcortical auditory system with EEG

#### The scalp-recorded frequency-following response (FFR) shows the fidelity of incoming sound encoding
![](images/Coffey2018_Fig1a.png){fig-align="center" .fragment}

::: aside
Coffey et al. (2018)
:::

::: {.notes}
:::

## *Where* are motor signals integrated? Subcortically? 

![](images/people/GabbyButler.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

:::: {layout="[60,40]"}

::: {.column .fragment}
### If integration is subcortical, would expect to see FFR differences between self-generated vs. passive sound
- Recorded FFRs to /da/ stimulus (alternating polarity)
  - *Active* condition: press a button to generate a sound
  - *Passive* condion: passive presentation of the sound
- Collected data from 33 normal-hearing adults while they watched a silent captioned TV show
:::

::: {.column .fragment}

![](images/Butler_etal_2025/Butler_EEG.png){fig-align="left" height="300"}

![](images/Butler_etal_2025/Butler_stimulus.png){fig-align="left" height="250"}

:::
::::

::: aside
Butler, ..., Sitek (in prep; SNL 2025)
:::

::: {.notes}
If integration is subcortical, would expect to see FFR differences between self-generated vs. passive sound
:::

## Active and passive evoked responses

![](images/people/GabbyButler.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

:::: {.columns}

::: {.column .fragment}
#### Brainstem frequency-following response
![](images/Butler_etal_2025/Butler_FFR_waveform.png){height=700}
:::

::: {.column .fragment}
#### Cortical evoked response
![](images/Butler_etal_2025/Butler_cortical_waveform.png){height=700}

:::
::::

::: aside
Butler, ..., Sitek (in prep; SNL 2025)
:::

::: {.notes}
:::

## Active and passive spectrograms {visibility="hidden"}

![](images/people/GabbyButler.jpeg){.absolute bottom=10 right=10 width="200" height="200"}


:::: {.columns}

::: {.column .fragment}
#### Brainstem frequency-following response
![](images/Butler_etal_2025/Butler_FFR_spectro.png){fig-align="center" height=450}
:::

::: {.column .fragment}
#### Cortical evoked response
![](images/Butler_etal_2025/Butler_cortical_spectro.png){fig-align="center" height=450}

:::
::::

::: aside
Butler, ..., Sitek (in prep; SNL 2025)
:::

::: {.notes}
:::

## Active and passive pitch tracking 

![](images/people/GabbyButler.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

![](images/Butler_etal_2025/Butler_FFR_pitchtracking.png){.fragment height="750" fig-align="center"}

::: aside
Butler, ..., Sitek (in prep; SNL 2025)
:::

::: {.notes}
:::

## *Where* are motor signals integrated? Subcortically?

![](images/people/GabbyButler.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

#### Brainstem frequency-following response (FFR) shows the same sound encoding when a sound is self-generated or passively presented

::: {.incremental}
- If integration is subcortical, would expect to see brainstem FFR differences between self-generated vs. passive sound
- No brainstem FFR differences, despite cortical evoked response differences
- **Motor signals unlikely to attenuate earliest central auditory signals**
- *Next steps*: Source localization of FFR generators (ECR R21 project)
:::

::: aside
Butler, ..., Sitek (in prep; SNL 2025)
:::

::: {.notes}
If integration is subcortical, would expect to see FFR differences between self-generated vs. passive sound
:::

## *Where* are motor signals integrated in auditory cortex?

::: {.fragment}
- Previous literature implicates auditory cortex as a hub for auditory–motor integration
- But fMRI has shown *increased* BOLD to self-generated sounds, vs. *suppressed* M/EEG responses^[Reznik et al. (2015)]
- Are different **laminar** responses the explanation? 
:::
::: {.fragment}
- We can look at intra-cortical circuits with ultra-high field **7T fMRI**
  - Depth-dependent BOLD fMRI acquisition
  - 7 participants: 1 mm isotropic voxels, TE = 22.8 ms, MB = 6
  - 1 replication: 0.8 mm isotropic voxels, TE = 21.4, MB = 4
  - Cortical depth profile differences during self-generated vs. passive sound
:::

![](images/Reznik2014_Figure1a.png){fig-align="center" height="250"}

::: aside
Sitek et al. (in prep; OHBM 2025)
:::

::: {.notes}
Speaker notes go here.
:::

## Greater self-generated vs. passive fMRI responses in a single participant across replications

:::: {.columns}
::: {.column .fragment}
![](images/Sitek_MIS_slices_contrast-listen.png)
:::

::: {.column .fragment}
![](images/Sitek_MIS_slices_contrast-selfgen-vs-listen.png)
:::
::::

::: aside
Sitek et al. (in prep; OHBM 2025)
:::

::: {.notes}
Speaker notes go here.
:::

## Stronger self-generated (vs. passive) sound responses in superficial auditory cortex

![](images/7T_replication.png)

::: aside
Sitek et al. (in prep; OHBM 2025)
:::

::: {.notes}
Speaker notes go here.
:::

## Evidence for layer-specific motor-related activation in human auditory cortex

:::: {.columns}

::: {.column width="50%" .fragment}
::: {.incremental}
- Increased BOLD in **self-generated** vs. passive condition
- Higher increases at **superficial** cortical depths, suggesting increased *corticocortical* feedback
- Helps explain increased BOLD in previous studies
- Reproducible---same cortical depth effects in two acquisitions in the same participant
:::
:::

::: {.column width="50%"}
![](images/Sitek_MIS_slices.png){height="400"}
![](images/7T_replication.png){height="400"}
:::
::::

::: aside
Sitek et al. (in prep; OHBM 2025)
:::

::: {.notes}
Speaker notes go here.
:::

# Wrapping up

## Non-invasively investigating the human auditory brainstem

:::: {.columns layout-valign="center" }
::: {.column width="33%" .fragment}
### Mapping the human subcortical auditory system
![](images/SitekGulban2019/fig_invivo_diffusion-only.png)
:::

::: {.column width="33%" .fragment}
### Auditory–motor interactions across the auditory pathway
![](images/Butler_etal_2025/Butler_FFR_pitchtracking-minimal.png)
:::

::: {.column width="33%" .fragment}
### Auditory categorization in human dorsal striatum
![](images/Sitek2025_categorization/Sitek_feedback_striatal_all-runs_minimal.png)
:::
::::

::: {.notes}
Speaker notes go here.
:::

## Acknowledgments {.smaller}

:::: {.columns}

::: {.column width="50%"}

#### SoundBrain Lab
- Bharath Chandrasekaran, Jacie McHaney, Kailyn McFarlane
- Nike Gnanateja (now UW Madison)
- Casey Roark (now U New Hampshire)

![](images/people/SoundBrain-lab-current-members-and-alum_FFR-workshop-2024-550x310.jpg){height="500"}

:::

::: {.column width=50%"}
**Northwestern**: Molly Bright, Michelle Medina (*BME PhD student*), Gabby Butler (Cog Sci UG honors student)

**UT Austin collaborators**: Abhra Sarkar, Noirrit Chandra (*now UT Dallas Statistics*), Blake Moya (*now in industry*)

**Pitt**: Jay Bohland, Mandy Hampton Wray, Tamer Ibrahim (*K01 co-mentor*)

**Harvard/MIT**: Satra Ghosh (*PhD advisor*)

#### Active funding and support
- K01 DC019421 (2023–2027)
- R21 DC022906 (2025–2028)
- R01 DC020963 (2024–2027; PI: Bohland)

![](images/US-NIH-NIDCD-Logo.svg){fig-align="center" height=100}
:::

::::

::: {.notes}
Speaker notes go here.
:::

# Thank you! {background-image="/images/KevinSitek_postmortem-human-brainstem_auditory-tractography.png" background-opacity=0.25 background-size='contain'}

# Extra slides

::: {.notes}
Speaker notes go here.
:::


## How is the brainstem supplied oxygen? {.nostretch}

![](images/Xu2025/vasculature_T2w.png){.absolute top=10 right=10 width="300" height="300"}

::: {layout="[40,20,40]"  layout-valign="center" }
![Vessel ink staining (Duvernoy 1978)](images/Xu2025/vasculature_Duvernoy.png){height="750"}

![T2*-w MRI maximum intensity projection](images/Xu2025/vasculature_MIP.png){height="750"}

{{< video images/Xu2025/S64520_m0_SLA_booster_result_bg-white.mp4  >}}
:::

![](images/people/MarshallXu.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
OHBM 2023, 2024, 2025
:::

::: {.notes}
Speaker notes go here.
:::


## Finger motor representation in the human brain {.smaller}

::: {layout-ncol=3 .fragment}
![](images/Moya_etal_2025/dmncADL.png)

![](images/Moya_etal_2025/dmncADR.png)

![](images/Moya_etal_2025/dmncPVR.png)
:::

Posterior mean dominance greater than 25% across voxels for the [right index  (red)]{style="color:red;"}, [left index (blue)]{style="color:blue;"}, and [middle fingers (green)]{style="color:green;"}. 

![](images/people/BlakeMoya.jpeg){.absolute bottom=10 right=10 width="200" height="200"}

::: aside
Moya et al. (in prep.)
:::

::: {.notes}
Moya et al.
Results for the fingertip mapping study:     
Fingertip dominance across the investigated regions. Posterior mean dominance greater than $25\%$ is shown across voxels for the right index finger (red), left index (blue), and middle fingers (green) based on the estimated partition structures and the estimated effect intensities.     
Colors blend when two or more fingers exhibit greater than $25\%$ dominance probabilities at the same location,     showing areas of competing dominance for the right index and either middle finger (yellow), left index and either middle finger (cyan), and the two index fingers (purple).
:::


## The developing auditory system

#### How do we learn to hear in noisy environments?

::: {layout-ncol=2  layout-valign="bottom"}

::: {.incremental}

- Adults: maturation of speech network^[Du et al. (2014)] + compensatory networks^[Vaden et al. (2013); Eckert et al. (2016)]
- Children: fine-tuning speech network + undeveloped frontal networks
- Collected syllable-in-noise task fMRI at 3T in 35 typically developing kids 
  - 5 speech-shaped noise contexts: Quiet and SNR = +8, 0, -2, and -6
  - Task: identify which of 4 syllables were presented
:::

![via The Telegraph](images/Cocktail-party.jpg){fig-align="center"}

:::

::: aside
Sitek et al. (in prep.; SNL 2025)
:::

::: {.notes}
SSP
:::

## The developing auditory system: Speech-in-quiet vs. baseline fMRI contrast

![](images/Sitek_SSP_quiet-gt-baseline.png){fig-align="center" .fragment}

::: aside
Sitek et al. (in prep.; SNL 2025)
:::

::: {.notes}
SSP
:::

## The developing auditory system: Speech-in-quiet vs. -6 dB SNR

![](images/Sitek_SSP_quiet-gt-n6.png){fig-align="center"}

::: aside
Sitek et al. (in prep.; SNL 2025)
:::

::: {.notes}
SSP
:::

## Systems for speech-in-noise processing are actively developing in adolescents

::: {layout="[40,60]"   layout-valign="bottom"}
::: {.incremental}
- Children more strongly activate broad canonical auditory and language regions in clear speech compared to noisy stimuli
- No evidence that they utilize compensatory motor and prefrontal regions seen in adults
- Investigating task-relevant *representations* in development
- Currently collecting data in children with speech and language disorders
:::

![](images/Sitek_SSP_quiet-gt-n6.png){fig-align="center"}
:::

::: aside
Sitek et al. (in prep.; SNL 2025); 

Hampton-Wray R01DC019904
:::

::: {.notes}
SSP
:::

::: {.notes}
Speaker notes go here.
:::

## Future directions: Hearing loss {visibility="hidden"}

:::: {.columns}

::: {.column width="50%"}

### Speech-in-noise difficulties in middle age 
*put an image here!*

:::

::: {.column width=50%"}
### Neural predictors of cochlear implant success
*put an image here!*
:::

::::

::: aside
:::

::: {.notes}
:::

## Future directions: Analysis methods {visibility="hidden"}

:::: {.columns}

::: {.column width="50%"}

### `ffrprep`
*put an image here!*

:::

::: {.column width=50%"}
### Ex 2
*put an image here!*
:::

::::

::: aside
:::

::: {.notes}
:::


# Communication in practice: Sharing our science {visibility="hidden"}

::: {.notes}
Speaker notes go here.
:::

## Science is not a silo {visibility="hidden"}

:::: {.columns}

::: {.column width="50%"}

Communicating our science:

- builds community
- strengthens impact
- increases resilience
:::

::: {.column width=50%"}
*put an image here!*
:::

::::

::: aside
Sitek et al. (2025 *Aperture Neuro*); 
Wearn, Sitek, et al. (2025 *Aperture Neuro*)
:::

::: {.notes}
Sitek et al. (2025 *Aperture Neuro*); 
Wearn, Sitek, et al. (2025 *Aperture Neuro*)
:::

